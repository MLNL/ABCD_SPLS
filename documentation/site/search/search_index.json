{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PLS/CCA Toolkit This is a toolkit to incorporate standard Canonical Correlation Analysis (CCA), Partial Least Squares (PLS) and their regularized variants to investigate multivariate associations between multiple modalities of data, e.g. brain imaging and behaviour or brain imaging and genetics. The toolkit includes various options for PLS/CCA models (e.g. PLS, CCA, PCA-CCA, SPLS, RCCA), statistical inference (descriptive or predictive), optimizing hyperparameters for regularized PLS/CCA as well as others (e.g. different deflation strategies). Important features of the toolkit: testing the generalizablity of the associative effects using mutiple holdouts testing the stability of models during regularization hyperparameter selection iterative calculation of associative effects, i.e. optimal regularization for each associative effect For a discussion of the importance on pipeline choices, see Mihalik et al. 2020a . Applications Monteiro, J.M., Rao, A., Shawe-Taylor, J. & Mourao-Miranda, J. (2016) A multiple hold-out framework for Sparse Partial Least Squares. J. Neurosci. Methods 271, 182-194. Mihalik, A., Ferreira, F.S., Rosa, M.J. et al. (2019) Brain-behaviour modes of covariation in healthy and clinically depressed young people. Sci. Rep. 9, 11536. Mihalik, A., Ferreira, F.S, Moutoussis, M. et al. (2020) Multiple Holdouts With Stability: Improving the Generalizability of Machine Learning Analyses of Brain-Behavior Relationships Biol. Psychiatry 87, 368-376. Warning Please note that this toolkit is still under active development. Use wisely! Introduction to CCA/PLS models Notations: \\mathbf{X} \\mathbf{X} and \\mathbf{Y} \\mathbf{Y} are matrices for data modalities containing one (standardized) variable/feature per column and one example/sample per row ||\\mathbf{w}||_2 ||\\mathbf{w}||_2 is the L2-norm of a vector \\mathbf{w} \\mathbf{w} ||\\mathbf{w}||_1 ||\\mathbf{w}||_1 is the L1-norm of a vector \\mathbf{w} \\mathbf{w} Canonical Correlation Analysis (CCA) CCA finds a pair of weights, \\mathbf{u} \\mathbf{u} and \\mathbf{v} \\mathbf{v} , such that the correlation between the projections of \\mathbf{X} \\mathbf{X} and \\mathbf{Y} \\mathbf{Y} onto these weights are maximised: $$ max_{\\mathbf{u},\\mathbf{v}}\\mathbf{u^TX^TYv}\\ \\text{subject to } ||\\mathbf{Xu}||_2^2 = 1,\\ ||\\mathbf{Yv}||_2^2 = 1 $$ Partial Least Squares (PLS) PLS finds a pair of weights, \\mathbf{u} \\mathbf{u} and \\mathbf{v} \\mathbf{v} , such that the covariance between the projections of \\mathbf{X} \\mathbf{X} and \\mathbf{Y} \\mathbf{Y} onto these weights are maximised: $$ max_{\\mathbf{u},\\mathbf{v}}\\mathbf{u^TX^TYv}\\ \\text{subject to } ||\\mathbf{u}||_2^2 = 1,\\ ||\\mathbf{v}||_2^2 = 1 $$ Sparse PLS (SPLS) PLS finds a pair of weights, \\mathbf{u} \\mathbf{u} and \\mathbf{v} \\mathbf{v} , such that the covariance between the projections of \\mathbf{X} \\mathbf{X} and \\mathbf{Y} \\mathbf{Y} onto these weights are maximised, whilst enforcing sparse weights (i.e. selecting a subset of variables): $$ max_{\\mathbf{u},\\mathbf{v}}\\mathbf{u^TX^TYv}\\ \\text{subject to } ||\\mathbf{u}||_2^2 = 1,\\ ||\\mathbf{u}||_1 \\le c_u,\\ ||\\mathbf{v}||_2^2 = 1,\\ ||\\mathbf{v}||_1 \\le c_v\\ $$ To Do Add some basic information on regularized CCA (RCCA). Introduction to iterative solution of CCA/PLS models Although there are standard methods to estimate all multivariate associative effects or weights for CCA/PLS at once, it is desired to have an iterative solution for three main reasons: to choose weights of the most representative data split, to apply constraints between the associative effects in a flexible way (e.g. desirable orthogonality properties), to optimize the regularization hyperparameters for each associative effect independently. In such iterative solution, one pair of weights is estimated at a time and then removed from the data (by a process known as deflation ). Then the same process is repeated to find consecutive pairs of weights. CCA/PLS models and their regularized variants can be divided in two main groups based on their modelling aim. CCA/PLS can be used either for identifying associations between two modalities of data (symmetric variants) or to predict one modality from the other ones (asymmetric variants). Depending on the deflation strategy there are different types of symmetric ( CCA , RCCA , PLS Mode A , PLS-SVD ) and asymmetric ( PLS1/PLS2 , where PLS1 refers to the variant with a single output variable and PLS2 refers to the variant with multiple output variables) variants . CCA, RCCA and PLS-SVD can be seen as subcases of the generalized eigenvalue problem, therefore they inherit their deflations from the deflation of the generalized eigenvalue problem (called generalized deflation). The generalized eigenvalue problem is defined as: $$ \\mathbf{Aw}=\\lambda\\mathbf{Bw} $$ with \\mathbf{A} \\mathbf{A} , \\mathbf{B} \\mathbf{B} symmetric matrices, \\mathbf{B} \\mathbf{B} positive definite. \\mathbf{A} \\mathbf{A} contains the variance matrices (e.g. \\mathbf{C_{xx}} \\mathbf{C_{xx}} and \\mathbf{C_{yy}} \\mathbf{C_{yy}} ) on the diagonal and is zero elsewhere, \\mathbf{B} \\mathbf{B} contains the covariance matrices (e.g. \\mathbf{C_{xy}} \\mathbf{C_{xy}} and \\mathbf{C_{yx}} \\mathbf{C_{yx}} ) on the off-diagonal and is zero elswhere, \\mathbf{w} \\mathbf{w} contains the weights for both data modalities (i.e., \\mathbf{w_x} \\mathbf{w_x} and \\mathbf{w_y} \\mathbf{w_y} ) as a column vector per associative effect. The generalized deflation on the variance/covariance matrices is as follows: $$ \\mathbf{A}\\leftarrow\\mathbf{A}-\\lambda\\mathbf{B}\\mathbf{w}\\ \\mathbf{w^T}\\mathbf{B} $$ Notably, deflation of \\mathbf{B} \\mathbf{B} is not needed. The generalized deflation can be written in the input data space as: $$ \\mathbf{X}\\leftarrow\\mathbf{X}-\\mathbf{X}\\mathbf{w_x}\\ \\mathbf{w_x^T}\\mathbf{B_{xx}} $$ $$ \\mathbf{Y}\\leftarrow\\mathbf{Y}-\\mathbf{Y}\\mathbf{w_y}\\ \\mathbf{w_y^T}\\mathbf{B_{yy}} $$ and we refer to this method of deflation as projection deflation . In case of CCA ( cca-projection ), \\mathbf{B_{xx}} \\mathbf{B_{xx}} and \\mathbf{B_{yy}} \\mathbf{B_{yy}} are defined as: $$ \\mathbf{B_{xx}} = \\mathbf{C_{xx}} $$ $$ \\mathbf{B_{yy}} = \\mathbf{C_{yy}} $$ where \\mathbf{C_{xx}} \\mathbf{C_{xx}} and \\mathbf{C_{yy}} \\mathbf{C_{yy}} are the covariance matrices of \\mathbf{X} \\mathbf{X} and \\mathbf{Y} \\mathbf{Y} , respectively. In case of RCCA ( cca-projection ), \\mathbf{B_{xx}} \\mathbf{B_{xx}} and \\mathbf{B_{yy}} \\mathbf{B_{yy}} are defined as: $$ \\mathbf{B_{xx}} = (1-\\tau_{x})\\mathbf{C_{xx}}+\\tau_{x}\\mathbf{I} $$ $$ \\mathbf{B_{yy}} = (1-\\tau_{y})\\mathbf{C_{yy}}+\\tau_{y}\\mathbf{I} $$ where \\tau_{x} \\tau_{x} and \\tau_{y} \\tau_{y} are the L2 regularization hyperparameters. In case of PLS-SVD ( pls-projection ), \\mathbf{B_{xx}} \\mathbf{B_{xx}} and \\mathbf{B_{yy}} \\mathbf{B_{yy}} are defined as: $$ \\mathbf{B_{xx}} = \\mathbf{I} $$ $$ \\mathbf{B_{yy}} = \\mathbf{I} $$ For further details on the generalized eigenvalue problem and the generalized deflation, see Shawe-Taylor & Cristiani 2004 . The deflation for PLS Mode A ( pls-modeA ) is as follows: $$ \\mathbf{X}\\leftarrow\\mathbf{X}-\\mathbf{X}\\mathbf{w_x}\\ \\mathbf{p_x^T} $$ $$ \\mathbf{Y}\\leftarrow\\mathbf{Y}-\\mathbf{Y}\\mathbf{w_y}\\ \\mathbf{p_y^T} $$ where \\mathbf{p_x}=\\frac{\\mathbf{X^TXw_x}}{\\mathbf{w_x^TX^Xw_x}} \\mathbf{p_x}=\\frac{\\mathbf{X^TXw_x}}{\\mathbf{w_x^TX^Xw_x}} and \\mathbf{p_y}=\\frac{\\mathbf{Y^TYw_y}}{\\mathbf{w_y^TY^Yw_y}} \\mathbf{p_y}=\\frac{\\mathbf{Y^TYw_y}}{\\mathbf{w_y^TY^Yw_y}} . The deflation for PLS1/PLS2 ( pls-regression ) is the same as for PLS Mode A, but it is applied only on the predictor data, commonly denoted by \\mathbf{X} \\mathbf{X} , in line with the asymetric nature of this PLS variant. For further details on CCA/PLS models and deflations, see the Supplemental Information of Mihalik et al. 2020b . Contributors Agoston Mihalik Fabio Ferreira Nils Winter","title":"Home"},{"location":"#plscca-toolkit","text":"This is a toolkit to incorporate standard Canonical Correlation Analysis (CCA), Partial Least Squares (PLS) and their regularized variants to investigate multivariate associations between multiple modalities of data, e.g. brain imaging and behaviour or brain imaging and genetics. The toolkit includes various options for PLS/CCA models (e.g. PLS, CCA, PCA-CCA, SPLS, RCCA), statistical inference (descriptive or predictive), optimizing hyperparameters for regularized PLS/CCA as well as others (e.g. different deflation strategies). Important features of the toolkit: testing the generalizablity of the associative effects using mutiple holdouts testing the stability of models during regularization hyperparameter selection iterative calculation of associative effects, i.e. optimal regularization for each associative effect For a discussion of the importance on pipeline choices, see Mihalik et al. 2020a .","title":"PLS/CCA Toolkit"},{"location":"#applications","text":"Monteiro, J.M., Rao, A., Shawe-Taylor, J. & Mourao-Miranda, J. (2016) A multiple hold-out framework for Sparse Partial Least Squares. J. Neurosci. Methods 271, 182-194. Mihalik, A., Ferreira, F.S., Rosa, M.J. et al. (2019) Brain-behaviour modes of covariation in healthy and clinically depressed young people. Sci. Rep. 9, 11536. Mihalik, A., Ferreira, F.S, Moutoussis, M. et al. (2020) Multiple Holdouts With Stability: Improving the Generalizability of Machine Learning Analyses of Brain-Behavior Relationships Biol. Psychiatry 87, 368-376. Warning Please note that this toolkit is still under active development. Use wisely!","title":"Applications"},{"location":"#introduction-to-ccapls-models","text":"Notations: \\mathbf{X} \\mathbf{X} and \\mathbf{Y} \\mathbf{Y} are matrices for data modalities containing one (standardized) variable/feature per column and one example/sample per row ||\\mathbf{w}||_2 ||\\mathbf{w}||_2 is the L2-norm of a vector \\mathbf{w} \\mathbf{w} ||\\mathbf{w}||_1 ||\\mathbf{w}||_1 is the L1-norm of a vector \\mathbf{w} \\mathbf{w}","title":"Introduction to CCA/PLS models"},{"location":"#canonical-correlation-analysis-cca","text":"CCA finds a pair of weights, \\mathbf{u} \\mathbf{u} and \\mathbf{v} \\mathbf{v} , such that the correlation between the projections of \\mathbf{X} \\mathbf{X} and \\mathbf{Y} \\mathbf{Y} onto these weights are maximised: $$ max_{\\mathbf{u},\\mathbf{v}}\\mathbf{u^TX^TYv}\\ \\text{subject to } ||\\mathbf{Xu}||_2^2 = 1,\\ ||\\mathbf{Yv}||_2^2 = 1 $$","title":"Canonical Correlation Analysis (CCA)"},{"location":"#partial-least-squares-pls","text":"PLS finds a pair of weights, \\mathbf{u} \\mathbf{u} and \\mathbf{v} \\mathbf{v} , such that the covariance between the projections of \\mathbf{X} \\mathbf{X} and \\mathbf{Y} \\mathbf{Y} onto these weights are maximised: $$ max_{\\mathbf{u},\\mathbf{v}}\\mathbf{u^TX^TYv}\\ \\text{subject to } ||\\mathbf{u}||_2^2 = 1,\\ ||\\mathbf{v}||_2^2 = 1 $$","title":"Partial Least Squares (PLS)"},{"location":"#sparse-pls-spls","text":"PLS finds a pair of weights, \\mathbf{u} \\mathbf{u} and \\mathbf{v} \\mathbf{v} , such that the covariance between the projections of \\mathbf{X} \\mathbf{X} and \\mathbf{Y} \\mathbf{Y} onto these weights are maximised, whilst enforcing sparse weights (i.e. selecting a subset of variables): $$ max_{\\mathbf{u},\\mathbf{v}}\\mathbf{u^TX^TYv}\\ \\text{subject to } ||\\mathbf{u}||_2^2 = 1,\\ ||\\mathbf{u}||_1 \\le c_u,\\ ||\\mathbf{v}||_2^2 = 1,\\ ||\\mathbf{v}||_1 \\le c_v\\ $$ To Do Add some basic information on regularized CCA (RCCA).","title":"Sparse PLS (SPLS)"},{"location":"#introduction-to-iterative-solution-of-ccapls-models","text":"Although there are standard methods to estimate all multivariate associative effects or weights for CCA/PLS at once, it is desired to have an iterative solution for three main reasons: to choose weights of the most representative data split, to apply constraints between the associative effects in a flexible way (e.g. desirable orthogonality properties), to optimize the regularization hyperparameters for each associative effect independently. In such iterative solution, one pair of weights is estimated at a time and then removed from the data (by a process known as deflation ). Then the same process is repeated to find consecutive pairs of weights. CCA/PLS models and their regularized variants can be divided in two main groups based on their modelling aim. CCA/PLS can be used either for identifying associations between two modalities of data (symmetric variants) or to predict one modality from the other ones (asymmetric variants). Depending on the deflation strategy there are different types of symmetric ( CCA , RCCA , PLS Mode A , PLS-SVD ) and asymmetric ( PLS1/PLS2 , where PLS1 refers to the variant with a single output variable and PLS2 refers to the variant with multiple output variables) variants . CCA, RCCA and PLS-SVD can be seen as subcases of the generalized eigenvalue problem, therefore they inherit their deflations from the deflation of the generalized eigenvalue problem (called generalized deflation). The generalized eigenvalue problem is defined as: $$ \\mathbf{Aw}=\\lambda\\mathbf{Bw} $$ with \\mathbf{A} \\mathbf{A} , \\mathbf{B} \\mathbf{B} symmetric matrices, \\mathbf{B} \\mathbf{B} positive definite. \\mathbf{A} \\mathbf{A} contains the variance matrices (e.g. \\mathbf{C_{xx}} \\mathbf{C_{xx}} and \\mathbf{C_{yy}} \\mathbf{C_{yy}} ) on the diagonal and is zero elsewhere, \\mathbf{B} \\mathbf{B} contains the covariance matrices (e.g. \\mathbf{C_{xy}} \\mathbf{C_{xy}} and \\mathbf{C_{yx}} \\mathbf{C_{yx}} ) on the off-diagonal and is zero elswhere, \\mathbf{w} \\mathbf{w} contains the weights for both data modalities (i.e., \\mathbf{w_x} \\mathbf{w_x} and \\mathbf{w_y} \\mathbf{w_y} ) as a column vector per associative effect. The generalized deflation on the variance/covariance matrices is as follows: $$ \\mathbf{A}\\leftarrow\\mathbf{A}-\\lambda\\mathbf{B}\\mathbf{w}\\ \\mathbf{w^T}\\mathbf{B} $$ Notably, deflation of \\mathbf{B} \\mathbf{B} is not needed. The generalized deflation can be written in the input data space as: $$ \\mathbf{X}\\leftarrow\\mathbf{X}-\\mathbf{X}\\mathbf{w_x}\\ \\mathbf{w_x^T}\\mathbf{B_{xx}} $$ $$ \\mathbf{Y}\\leftarrow\\mathbf{Y}-\\mathbf{Y}\\mathbf{w_y}\\ \\mathbf{w_y^T}\\mathbf{B_{yy}} $$ and we refer to this method of deflation as projection deflation . In case of CCA ( cca-projection ), \\mathbf{B_{xx}} \\mathbf{B_{xx}} and \\mathbf{B_{yy}} \\mathbf{B_{yy}} are defined as: $$ \\mathbf{B_{xx}} = \\mathbf{C_{xx}} $$ $$ \\mathbf{B_{yy}} = \\mathbf{C_{yy}} $$ where \\mathbf{C_{xx}} \\mathbf{C_{xx}} and \\mathbf{C_{yy}} \\mathbf{C_{yy}} are the covariance matrices of \\mathbf{X} \\mathbf{X} and \\mathbf{Y} \\mathbf{Y} , respectively. In case of RCCA ( cca-projection ), \\mathbf{B_{xx}} \\mathbf{B_{xx}} and \\mathbf{B_{yy}} \\mathbf{B_{yy}} are defined as: $$ \\mathbf{B_{xx}} = (1-\\tau_{x})\\mathbf{C_{xx}}+\\tau_{x}\\mathbf{I} $$ $$ \\mathbf{B_{yy}} = (1-\\tau_{y})\\mathbf{C_{yy}}+\\tau_{y}\\mathbf{I} $$ where \\tau_{x} \\tau_{x} and \\tau_{y} \\tau_{y} are the L2 regularization hyperparameters. In case of PLS-SVD ( pls-projection ), \\mathbf{B_{xx}} \\mathbf{B_{xx}} and \\mathbf{B_{yy}} \\mathbf{B_{yy}} are defined as: $$ \\mathbf{B_{xx}} = \\mathbf{I} $$ $$ \\mathbf{B_{yy}} = \\mathbf{I} $$ For further details on the generalized eigenvalue problem and the generalized deflation, see Shawe-Taylor & Cristiani 2004 . The deflation for PLS Mode A ( pls-modeA ) is as follows: $$ \\mathbf{X}\\leftarrow\\mathbf{X}-\\mathbf{X}\\mathbf{w_x}\\ \\mathbf{p_x^T} $$ $$ \\mathbf{Y}\\leftarrow\\mathbf{Y}-\\mathbf{Y}\\mathbf{w_y}\\ \\mathbf{p_y^T} $$ where \\mathbf{p_x}=\\frac{\\mathbf{X^TXw_x}}{\\mathbf{w_x^TX^Xw_x}} \\mathbf{p_x}=\\frac{\\mathbf{X^TXw_x}}{\\mathbf{w_x^TX^Xw_x}} and \\mathbf{p_y}=\\frac{\\mathbf{Y^TYw_y}}{\\mathbf{w_y^TY^Yw_y}} \\mathbf{p_y}=\\frac{\\mathbf{Y^TYw_y}}{\\mathbf{w_y^TY^Yw_y}} . The deflation for PLS1/PLS2 ( pls-regression ) is the same as for PLS Mode A, but it is applied only on the predictor data, commonly denoted by \\mathbf{X} \\mathbf{X} , in line with the asymetric nature of this PLS variant. For further details on CCA/PLS models and deflations, see the Supplemental Information of Mihalik et al. 2020b .","title":"Introduction to iterative solution of CCA/PLS models"},{"location":"#contributors","text":"Agoston Mihalik Fabio Ferreira Nils Winter","title":"Contributors"},{"location":"example/","text":"In the following paragraph, we will go through a simple example, run an analysis and plot the results. We will be using Sparse Partial Least Squares (SPLS) in a multiple holdout framework. Our settings will be added to structures called cfg and res that will control the behaviour of the analysis and the results, respectively. Warning We will provide some example data in the future that everyone can use in this example. Until that point, please make sure to have some data ready that you can use here. A more detailed description of how the data should look like will come shortly. Basic analysis First, make sure to run set_path to add the necessary paths of the toolkit to your MATLAB path. set_path Project Folder Next, we specify the folder to our project. Make sure to specify the correct path. This folder will contain a data and a framework folder. Unless you run simulations, you should provide your data matrices ( X.mat , Y.mat ) in your data folder. In the following, we will assume that mat files and their content always has the same naming convention, i.e. X variable in X.mat . You can provide a data matrix for confounds ( C.mat ) which will be regressed out from both of your X and Y data. You can also provide a matrix ( EB.mat ) which defines the exchangeability block structure of your data and will be used for stratified partitioning of the data (i.e., inner/outer splits) and restricted permutations. For instance, you can use this to provide the genetic dependencies of your data (e.g. twins, family structure) or different cohorts (e.g. healthy vs. depressed sample). For details on how to create the \u00c8B matrix, see Winkler et al 2015 and the PALM toolbox . The framework folder will be automatically created by the toolkit in running time and will be filled up with all the results of your specific experiment. You can run different experiments on the same data, each having their own folder within framework , but we advise to create a new project folder once you change your data itself. cfg . dir . project = '/PATH/TO/YOUR/PROJECT/' ; Algorithm Now, we configure the algorithm we would like to use. We set the machine.name parameter to 'rcca' for Regularized CCA (RCCA). To select the best regularization hyperparameter (i.e. L2 regularization in case of RCCA), we will use test correlation as criterion (measuring the generalizability of the RCCA models across the inner loop splits). % Machine settings cfg . machine . name = 'rcca' ; cfg . machine . param . crit = 'correl' ; For more information on the machines and the regularization parameter choices, see here or the Home page and consult the references there. Environment Next, we set the computational environment for the toolkit. As RCCA is computationally efficient, often we can run it locally on our computer. % Environment settings cfg . env . comp = 'local' ; Framework Next, we set the framework to holdout to perform a multiple holdout approach. The frwork.flag parameter just defines a custom name for this analysis. Make sure to give it a name that will help you organize different analyses you might run on your data. % Framework settings cfg . frwork . name = 'holdout' ; cfg . frwork . flag = '_corr' ; For further details on the framework choices, see here . Deflation As we are optimizing the L2 regularization parameter of the RCCA model for each associative effect separately, we will need to remove the significant effects already found in the data (called deflation) to be able to find new associative effects. In this example, we will use CCA-projection deflation on all outer splits of the data using the model weight of the best split. For the definition of best , the value of the field cfg.defl.crit is used as a metric, e.g. highest holdout correlation. % Deflation settings cfg . defl . name = 'cca-projection' ; cfg . defl . crit = 'correl' ; cfg . defl . split = 'all' ; For more information about the theory behind deflation, see here , for more about the settings, see here . Statistical Inference Finally, we will need to define how the statistical significance test is performed. This can either be done separately for each split or combined across splits. In this example, we will do a so-called omnibus test based on the holdout correlations defined by cfg.stat.split.crit . For that, a permutation test is performed for each outer split and the p-values are Bonferroni corrected for multiple comparisons. Thus the null hypothesis states that none of the outer splits are significant, thus rejecting this hypothesis means that there is a significant effect in at least on of the outer splits. % Statistical inference cfg . stat . split . crit = 'correl' ; cfg . stat . overall . crit = 'none' ; % Number of permutation tests cfg . stat . nperm = 100 ; For further details on the statistical inference, see here . Run Analysis To run the analysis, we simply update our cfg structure to add all necessary default values that we didn't explicitly define and then run the main function. % Update cfg with defaults cfg = cfg_defaults ( cfg ); main ( cfg ); Basic plotting Now that we've run our first analysis, let's plot some of the results. Most of the times, this is simply done by configuring plotting specific details in the res structure and then passing this structure as input to the plot_weight or plot_proj function with some additional parameters the define the plot type. Before we can do any plotting, we need to make sure that we've called set_path('plot') to add the plotting folder. % Set path with plotting folder set_path ( 'plot' ); Additionally, we will need to load the res structure, which includes some of the results and the paths to the specific results. In general, we advise you to plot your results on a local computer as it is often cumbersome and slow in a cluster environment. In this case update_dir should be called just before res_defaults . Whenever you call res_defaults , all necessary default values will be loaded to the res structure. % Required fields res . dir . frwork = '/PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/' ; res . frwork . level = 1 ; res . env . fileend = '_1' ; % Initialize res res = res_defaults ( res , 'load' ); To plot the latent space that has been learnt by the model, simply run plot_proj . As first argument, we need to pass the res structure that holds all necessary information on where to find the results. Then, we can specify the data modalities as cell array that will be used for the plotting. The next parameter defines the associative effect ('level') that should be used for the plot. In this example, we plot the latent space (projections) of X and Y for the first associative effect. We set the fourth input parameter to 'osplit' so that the training and test data of the outer split will be used for the plot. The following parameter defines which split is used (in this case we use the first split). The second to last input parameter can be used to color code different groups of the data. Please see the documentation of plot_proj for more details. The last parameter defines the plot type. In this example, we will create a simple 2d plot. % Plot data projections plot_proj ( res , { 'X' 'Y' }, 1 , 'osplit' , 1 , 'none' , '2d' ) Plotting weights will heavily depend on the kind of data that has been used in the analysis. For that reason, we only provide a very basic weight plot in this example that creates a stem plot. % Plot X weights as stem plot plot_weight ( res , 'Y' , 'simul' , 1 , 'stem' ); % Plot Y weights as stem plot plot_weight ( res , 'X' , 'simul' , 1 , 'stem' ); For further details on plotting weights, see plot_weight and the plotting templates.","title":"Example"},{"location":"example/#basic-analysis","text":"First, make sure to run set_path to add the necessary paths of the toolkit to your MATLAB path. set_path","title":"Basic analysis"},{"location":"example/#project-folder","text":"Next, we specify the folder to our project. Make sure to specify the correct path. This folder will contain a data and a framework folder. Unless you run simulations, you should provide your data matrices ( X.mat , Y.mat ) in your data folder. In the following, we will assume that mat files and their content always has the same naming convention, i.e. X variable in X.mat . You can provide a data matrix for confounds ( C.mat ) which will be regressed out from both of your X and Y data. You can also provide a matrix ( EB.mat ) which defines the exchangeability block structure of your data and will be used for stratified partitioning of the data (i.e., inner/outer splits) and restricted permutations. For instance, you can use this to provide the genetic dependencies of your data (e.g. twins, family structure) or different cohorts (e.g. healthy vs. depressed sample). For details on how to create the \u00c8B matrix, see Winkler et al 2015 and the PALM toolbox . The framework folder will be automatically created by the toolkit in running time and will be filled up with all the results of your specific experiment. You can run different experiments on the same data, each having their own folder within framework , but we advise to create a new project folder once you change your data itself. cfg . dir . project = '/PATH/TO/YOUR/PROJECT/' ;","title":"Project Folder"},{"location":"example/#algorithm","text":"Now, we configure the algorithm we would like to use. We set the machine.name parameter to 'rcca' for Regularized CCA (RCCA). To select the best regularization hyperparameter (i.e. L2 regularization in case of RCCA), we will use test correlation as criterion (measuring the generalizability of the RCCA models across the inner loop splits). % Machine settings cfg . machine . name = 'rcca' ; cfg . machine . param . crit = 'correl' ; For more information on the machines and the regularization parameter choices, see here or the Home page and consult the references there.","title":"Algorithm"},{"location":"example/#environment","text":"Next, we set the computational environment for the toolkit. As RCCA is computationally efficient, often we can run it locally on our computer. % Environment settings cfg . env . comp = 'local' ;","title":"Environment"},{"location":"example/#framework","text":"Next, we set the framework to holdout to perform a multiple holdout approach. The frwork.flag parameter just defines a custom name for this analysis. Make sure to give it a name that will help you organize different analyses you might run on your data. % Framework settings cfg . frwork . name = 'holdout' ; cfg . frwork . flag = '_corr' ; For further details on the framework choices, see here .","title":"Framework"},{"location":"example/#deflation","text":"As we are optimizing the L2 regularization parameter of the RCCA model for each associative effect separately, we will need to remove the significant effects already found in the data (called deflation) to be able to find new associative effects. In this example, we will use CCA-projection deflation on all outer splits of the data using the model weight of the best split. For the definition of best , the value of the field cfg.defl.crit is used as a metric, e.g. highest holdout correlation. % Deflation settings cfg . defl . name = 'cca-projection' ; cfg . defl . crit = 'correl' ; cfg . defl . split = 'all' ; For more information about the theory behind deflation, see here , for more about the settings, see here .","title":"Deflation"},{"location":"example/#statistical-inference","text":"Finally, we will need to define how the statistical significance test is performed. This can either be done separately for each split or combined across splits. In this example, we will do a so-called omnibus test based on the holdout correlations defined by cfg.stat.split.crit . For that, a permutation test is performed for each outer split and the p-values are Bonferroni corrected for multiple comparisons. Thus the null hypothesis states that none of the outer splits are significant, thus rejecting this hypothesis means that there is a significant effect in at least on of the outer splits. % Statistical inference cfg . stat . split . crit = 'correl' ; cfg . stat . overall . crit = 'none' ; % Number of permutation tests cfg . stat . nperm = 100 ; For further details on the statistical inference, see here .","title":"Statistical Inference"},{"location":"example/#run-analysis","text":"To run the analysis, we simply update our cfg structure to add all necessary default values that we didn't explicitly define and then run the main function. % Update cfg with defaults cfg = cfg_defaults ( cfg ); main ( cfg );","title":"Run Analysis"},{"location":"example/#basic-plotting","text":"Now that we've run our first analysis, let's plot some of the results. Most of the times, this is simply done by configuring plotting specific details in the res structure and then passing this structure as input to the plot_weight or plot_proj function with some additional parameters the define the plot type. Before we can do any plotting, we need to make sure that we've called set_path('plot') to add the plotting folder. % Set path with plotting folder set_path ( 'plot' ); Additionally, we will need to load the res structure, which includes some of the results and the paths to the specific results. In general, we advise you to plot your results on a local computer as it is often cumbersome and slow in a cluster environment. In this case update_dir should be called just before res_defaults . Whenever you call res_defaults , all necessary default values will be loaded to the res structure. % Required fields res . dir . frwork = '/PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/' ; res . frwork . level = 1 ; res . env . fileend = '_1' ; % Initialize res res = res_defaults ( res , 'load' ); To plot the latent space that has been learnt by the model, simply run plot_proj . As first argument, we need to pass the res structure that holds all necessary information on where to find the results. Then, we can specify the data modalities as cell array that will be used for the plotting. The next parameter defines the associative effect ('level') that should be used for the plot. In this example, we plot the latent space (projections) of X and Y for the first associative effect. We set the fourth input parameter to 'osplit' so that the training and test data of the outer split will be used for the plot. The following parameter defines which split is used (in this case we use the first split). The second to last input parameter can be used to color code different groups of the data. Please see the documentation of plot_proj for more details. The last parameter defines the plot type. In this example, we will create a simple 2d plot. % Plot data projections plot_proj ( res , { 'X' 'Y' }, 1 , 'osplit' , 1 , 'none' , '2d' ) Plotting weights will heavily depend on the kind of data that has been used in the analysis. For that reason, we only provide a very basic weight plot in this example that creates a stem plot. % Plot X weights as stem plot plot_weight ( res , 'Y' , 'simul' , 1 , 'stem' ); % Plot Y weights as stem plot plot_weight ( res , 'X' , 'simul' , 1 , 'stem' ); For further details on plotting weights, see plot_weight and the plotting templates.","title":"Basic plotting"},{"location":"getting_started/","text":"The following section will help you get started with your first analysis and some basic plotting. First, we will explain the dependencies of the toolkit and how to set the path. Second, we will introduce the cfg and res structures that define the settings of your analysis and plots, respectively. Third, will discuss how to use templates that are provided to run pre-defined basic analyses and plots. Finally, we will go through an example step by step and demonstrate what you need to do to run an analysis and plot results. Setting the path The toolkit relies on the following toolbox for permutation testing: PALM For Canonical Correlation Analysis (CCA) and Regularized Canonical Correlation Analysis (RCCA), the toolkit relies on the following toolbox: FSL Nets Finally, we use some additional toolboxes for advanced functionalities or to create specific plots. These include the following: SPM12 BrainNet Viewer Nilearn AAL cca-tutorial To know if you will need any of these toolboxes, check out the settings for the specific plot you would like to make here . All these toolboxes are stored in a dedicated folder. To create this folder, run the following line in the MATLAB command window: mkdir external Importantly, the external folder and its content is added to .gitignore and thus they are not version controlled by git. This is to enable flexible functionality of the toolkit, i.e. users can have different content in their external folders, and also to reduce the size of the toolkit. Now, we demonstrate how to use the toolkit with SPM12. This enables using nifti (.nii) files e.g. for reading a brain mask or an atlas to work with 3D weight images. For this, download SPM12 manually and copy the 'spm12' folder into the external folder, then finally add SPM12 to the MATLAB path using the following command: set_path ( 'spm12' ) To be able to use the plotting functionality, you need to use the following command: set_path ( 'plot' ) You can also pass multiple arguments to set_path . The order of the arguments is arbitrary. For instance, to add SPM12 and the plot folder at the same time, run the following command: set_path ( 'spm12' , 'plot' ) Cfg and res The cfg and res structures control the behaviour of your analysis and plots, respectively. Therefore it is of uttermost importance that you are familiar with their settings. They are automatically initialized or updated by cfg_defaults and res_defaults during the execution of the toolkit (see templates below). To get more information on all fields of the cfg and res structure, please see cfg defaults and res defaults . We advise to inspect the fields of these structures once they are initialized or updated to make sure you use the settings you intended to do. Templates The easiest way of getting familiar with the toolkit is by reusing some of the template files that we have provided in the templates folder. There are 3 types of templates: templates for submitting jobs in the cluster templates for running analysis templates for plotting results The type of template can be recognised by its name. You should not change anything within the templates themselves but copy and paste their content to a new file, which should be placed either in the toolkit folder or one of its subfolders. In the latter case, make sure that the path is added to the MATLAB path. Job templates These templates are highly specific to the scheduling system of the cluster you want to run your analysis or plot the results on. Check if one of our templates meet your needs, otherwise you will need to create one for yourself. Analysis templates These templates work in a similar fashion. First, set_path is called to add the necessary paths of the toolkit to your MATLAB path. Second, the configurations of an analysis is defined in the fields of cfg . Third, the cfg is updated with the default values to ensure a valid configuration structure. Finally, the function main is called to initiate the analysis. Plotting templates These templates work in a similar fashion. First, set_path('plot') is called to add the necessary paths of the toolkit to your MATLAB path including the plot folder. Second, res is set with some required fields to be able to load the results of the analysis. Before loading the results, the paths are updated in your saved cfg and res structures in case you are plotting the results on a different computer as the analyses was run on. Finally, res is passed to various plotting functions to create different plots of the results. Importantly, res controls the behaviour of the plots, so you will need to modify its fields if you want to change the default plotting options. For more information on the different templates we provide, please go to the template section in the navigation bar.","title":"Getting Started"},{"location":"getting_started/#setting-the-path","text":"The toolkit relies on the following toolbox for permutation testing: PALM For Canonical Correlation Analysis (CCA) and Regularized Canonical Correlation Analysis (RCCA), the toolkit relies on the following toolbox: FSL Nets Finally, we use some additional toolboxes for advanced functionalities or to create specific plots. These include the following: SPM12 BrainNet Viewer Nilearn AAL cca-tutorial To know if you will need any of these toolboxes, check out the settings for the specific plot you would like to make here . All these toolboxes are stored in a dedicated folder. To create this folder, run the following line in the MATLAB command window: mkdir external Importantly, the external folder and its content is added to .gitignore and thus they are not version controlled by git. This is to enable flexible functionality of the toolkit, i.e. users can have different content in their external folders, and also to reduce the size of the toolkit. Now, we demonstrate how to use the toolkit with SPM12. This enables using nifti (.nii) files e.g. for reading a brain mask or an atlas to work with 3D weight images. For this, download SPM12 manually and copy the 'spm12' folder into the external folder, then finally add SPM12 to the MATLAB path using the following command: set_path ( 'spm12' ) To be able to use the plotting functionality, you need to use the following command: set_path ( 'plot' ) You can also pass multiple arguments to set_path . The order of the arguments is arbitrary. For instance, to add SPM12 and the plot folder at the same time, run the following command: set_path ( 'spm12' , 'plot' )","title":"Setting the path"},{"location":"getting_started/#cfg-and-res","text":"The cfg and res structures control the behaviour of your analysis and plots, respectively. Therefore it is of uttermost importance that you are familiar with their settings. They are automatically initialized or updated by cfg_defaults and res_defaults during the execution of the toolkit (see templates below). To get more information on all fields of the cfg and res structure, please see cfg defaults and res defaults . We advise to inspect the fields of these structures once they are initialized or updated to make sure you use the settings you intended to do.","title":"Cfg and res"},{"location":"getting_started/#templates","text":"The easiest way of getting familiar with the toolkit is by reusing some of the template files that we have provided in the templates folder. There are 3 types of templates: templates for submitting jobs in the cluster templates for running analysis templates for plotting results The type of template can be recognised by its name. You should not change anything within the templates themselves but copy and paste their content to a new file, which should be placed either in the toolkit folder or one of its subfolders. In the latter case, make sure that the path is added to the MATLAB path.","title":"Templates"},{"location":"getting_started/#job-templates","text":"These templates are highly specific to the scheduling system of the cluster you want to run your analysis or plot the results on. Check if one of our templates meet your needs, otherwise you will need to create one for yourself.","title":"Job templates"},{"location":"getting_started/#analysis-templates","text":"These templates work in a similar fashion. First, set_path is called to add the necessary paths of the toolkit to your MATLAB path. Second, the configurations of an analysis is defined in the fields of cfg . Third, the cfg is updated with the default values to ensure a valid configuration structure. Finally, the function main is called to initiate the analysis.","title":"Analysis templates"},{"location":"getting_started/#plotting-templates","text":"These templates work in a similar fashion. First, set_path('plot') is called to add the necessary paths of the toolkit to your MATLAB path including the plot folder. Second, res is set with some required fields to be able to load the results of the analysis. Before loading the results, the paths are updated in your saved cfg and res structures in case you are plotting the results on a different computer as the analyses was run on. Finally, res is passed to various plotting functions to create different plots of the results. Importantly, res controls the behaviour of the plots, so you will need to modify its fields if you want to change the default plotting options. For more information on the different templates we provide, please go to the template section in the navigation bar.","title":"Plotting templates"},{"location":"installation/","text":"To install the PLS/CCA Toolkit, clone the repository from Github using the following command: git clone https://github.com/anaston/PLS_CCA_framework After the toolkit downloaded, go to the folder containing the toolkit and open MATLAB. In general, we advise you to run all commands from this folder. To initialize the toolkit, run the following line in the MATLAB command window: set_path As the toolkit have some dependencies, you will need to download 1 or 2 toolboxes to be able to run an analysis. For plotting the results, you might need some additional toolboxes too, depending on the particular plots. For details, see Getting Started .","title":"Installation"},{"location":"mfiles/cfg_defaults/","text":"cfg_defaults Set defaults in your configuration ( cfg ) structure which will define the settings of your analysis (e.g. machine, framework, significance testing). Use this function to update and add all necessary defaults to your cfg . If you defined anything in your cfg before calling the function, it won't overwrite those values. The path to the project folder should be always defined in your cfg or passed as varargin, otherwise the function throws an error. All the other fields are optional and can be filled up by cfg_defaults . No results will be stored in the cfg structure. See res_defaults for more information on results. Warning We strongly advise to inspect the output of cfg_defaults to make sure that the defaults are set as expected. Syntax: cfg = cfg_defaults(cfg, varargin) Inputs cfg [ cfg structure ] varargin [ name-value pairs ] additional parameters can be set via name-value pairs with dot notation supported (e.g., 'frwork.split.nout', 5) Outputs cfg [ cfg structure ] configuration structure that has been updated with defaults Examples % Example 1 cfg . dir . project = 'PATH/TO/YOUR/PROJECT1' cfg = cfg_defaults ( cfg ) % Example 2 cfg . dir . project = 'PATH/TO/YOUR/PROJECT2' cfg = cfg_defaults ( cfg , 'machine.name' , 'spls' , ... 'machine.metric' , { 'correl' 'simwx' 'simwy' }, 'machine.param.crit' , 'correl+simwxy' ) % Example 3 cfg = cfg_defaults ([], 'dir.project' , 'PATH/TO/YOUR/PROJECT3' , ... 'machine.name' , 'rcca' , ... 'machine.metric' , { 'correl' }, 'machine.param.crit' , 'correl' , 'frwork.split.nout' , 5 ) Fields and options You can find a description of all possible settings of the fields and subfields of cfg below. First parameter always indicates the default option. dir Essential paths to your project, framework and processed data. Include 'simulation' in your project folder if you want to generate simulated data using Uurtio's github repo . The project folder should include a 'data' folder where all the input data are stored. For running experiments, you need X.mat , Y.mat , and optionally C.mat , EB.mat . For plotting results, you need label files in a tabulated text format (e.g., LabelsX.xlsx , LabelsY.xlsx ), a mask file (e.g. mask.mat or mask.nii ) for VBM or connectivity data. Depending on your plots you might need additional files too. For details, see res_defaults . .project [ path ] full path to your project, such as 'PATH/TO/YOUR/PROJECT' .frwork [ path ] full path to your framework, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME' analysis name is generated from machine name and framework settings (including flag) .load [ path ] full path to your processed data, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/load' it stores information about data processing for computational efficiency including preprocessing (e.g. mean and scale of features, betas for deconfounding) and results of PCA if CCA/RCCA is used machine Algorithm to be used, its settings and information about hyperparameter optimization. Please make sure that you are familiar with the hyperparameter settings of the chosen algorithm, e.g. range and scale of hyperparameter values for grid search or number of PCA components. In the latter case, if not data driven PCA components are chosen, we highly recommend to inspect your data before chosing the right amount of PCA components (e.g. using assess_pca_comp.m). We also strongly encourage to use RCCA or SPLS as CCA and PLS are likely to overfit the data and/or loose sensitivity unless the number of examples greatly exceed the number of features. .name [ 'cca', 'rcca', 'pls', 'spls' ] name of the algorithm PCA is implicitly used in CCA/RCCA algorithms, so set machine name to CCA if you want to perform PCA-CCA analysis RCCA smooths between CCA and PLS defined by the L2 regularization hyperparameter (L2=0 for CCA, L2=1 for PLS) .metric [ cell array ] metrics that are used to evaluate the machine including 'correl' (test correlation), 'covar' (test covariance), 'simwx' (similarity of X weights), 'simwy' (similarity of Y weights), 'unsuc' (number of unsuccessful convergence for SPLS) .param.crit [ 'correl', 'correl+simwxy', 'correl+simwx+simwy', 'covar', 'freq' ] criterion for hyperparameter selection, we recommend using 'correl' or 'correl+simwxy' 'correl+simwxy' uses a 2D distance metric based on correlation and averaged similarity of weights 'correl+simwx+simwy' uses a 3D distance metric based on correlation and similarity of X weights and Y weights .param.name [ cell array ] name of the hyperparameters, including 'L1x' and 'L1y' for L1 regularization (for sparsity), 'L2x' and 'L2y' for L2 regularization (for stability), 'PCAx' and 'PCAy' for number of PCA components we note that L2 regularization can be used together with PCA .param.type [ 'factorial', 'matched' ] defines if grid search for hyperparameters should be a factorial combination of all hyperparameters or matching correspoding hyperparameters (in this case all should have the same length) .param.L1x, .param.L1y [ int or numeric array ] amount of L1 regularization for data X, Y when using SPLS if not provided, the function generates a linearly scaled numeric array using the number and range of hyperparameters (see below) .param.nL1x, .param.nL1y [ int --> 10 ] number of L1 regularization hyperparameters for data X, Y used in grid search .param.rangeL1x, .param.rangeL1y [ numeric array ] range of L1 regularization hyperparameters for data X, Y used in grid search for L1 regularization to be active, range should be between 1 and square root of the number of features, otherwise PLS is used .param.L2x, .param.L2y [ int ] amount of L2 regularization for data X, Y when using RCCA if set to 0 RCCA performs CCA, if set to 1 RCCA performs PLS use 1 - inverse of the number of features for a decent amount of regularization if not provided, the function generates a logarithmically scaled numeric array using the number and range of hyperparameters (see below) .param.nL2x, .param.nL2y [ int ] number of L2 regularization hyperparameters for data X, Y used in grid search .param.rangeL2x, .param.rangeL2y [ numeric array --> [0 1] ] range of L2 regularization hyperparameters for data X used in grid search .param.PCAx, .param.PCAy [ int ] number of PCA components for data X, Y if not provided, the function uses .eig.tol to deal with rank-deficiency .param.VARx, .param.VARy [ int --> 0.99 ] retained variance during PCA step of CCA/RCCA .eig.tol [ int --> 1e-10 ] eigenvalues smaller than tolerance are removed during PCA step of CCA/RCCA .conv.tol [ int --> 1e-5; ] tolerance during SPLS convergence .conv.maxiter [ int --> 100 ] maximum number of iterations during SPLS convergence frwork Details of framework with three main approaches. Divide data to training and test sets using multiple holdouts (see Monteiro et al 2016 ) by randomly subsampling subjects or standard cross-validation by applying folds. Otherwise, use a permutation approach without any data splitting (see Smith et al 2015 ). The default values will change depending on the type of the framework. .name [ 'holdout', 'cv', 'permutation' ] type of the framework .flag [ char ] a short name/flag to be appended to your analysis name which will then define the framework folder, see cfg.dir.frwork .split.nout [ int ] number of outer splits/folds .split.propout [ float --> 0.2 ] proportion of holdout/test set in 'holdout' framework higher value is recommended for samples n<500 (e.g. 0.2-0.5), and lower value (e.g. 0.1) should be sufficient for samples n > 1000 set to 0 for 'permutation' framework .split.nin [ int ] number of inner splits/folds .split.propin [ float --> 0.2 ] proportion of validation set in 'holdout' framework .split.EBcol [ int or numeric array ] indexes of columns in EB matrix to use for defining exchangeability blocks for data partitioning if multi-level blocks are provided, most likely you need to provide 2 columns here as e.g. no cross-over across different family types (column 2) but shuffling across families (column 3) within same family type are allowed, in other words, families should be in the same data split (training or test) defl Deflation types and strategies. For an in-depth introduction to deflation (i.e. iterative solution of CCA/PLS) and its different types, see Home . As most of the time we use different outer splits/folds of the data, it is of interest which split to use as the basis for deflation. A natural choice is to use the weights of the best data split (e.g. with highest holdout correlation and similarity across weights) and deflate all other splits with it. Other options are e.g. to deflate significant splits either by using the weights of the best split or use all splits independently. We note, that only the latest approach is in its pure sense a matrix decomposition (especially when standard CCA/PLS or the same regularization hyperparameter is used across associative effects) as in the other cases we mix effects across splits. Nevertheless, in practice we can gain robustness of the results by choosing e.g. best data split and/or deflating all data splits. .name [ 'cca-projection', 'pls-projection', 'pls-modeA', 'pls-regression' ] type of deflation .crit [ 'correl', 'pval+correl', 'correl+simwxy', 'correl+simwx+simwy', 'none' ] criterion to define best (i.e. most representative) data split to be used for deflation if 'none' set then each split is deflated by itself (i.e. they are treated independently) .split [ 'significant', 'all' ] selection of splits for deflation, i.e. whether all splits are deflated or only the significant ones (the latter approach needs significance testing within splits) stat Statistical inference. Our approach is to make inference about the multivariate associative effect and not necessarily about specific model weights unless sparsity is used to select features/variables. Testing the generalizability of the models (i.e., using test correlations) is one of our key recommendations. Furthermore, we advise to check the robustness (i.e., how many data splits are significant) and the stability (i.e., similarity of the weights) of the models and not just rely purely on p-values. There are two main statistical inferences in the toolkit. One is based on an omnibus hypothesis proposed by Monteiro et al 2016 which performs statistical inference both within and across splits. The other is based on statistical inference across splits and relies highly on the stability of models across outer splits (see Mihalik et al 2020 ). The first approach is more conservative and might produce false negative results (especially in datasets with <500 subjects), whilst the latter might be too permissive when working with large datasets (>1000 subjects). .nperm [ int ] number of permutations .split.crit [ 'correl', 'none' ] statistical inference within splits (i.e., 1 permutation test for each data split) based on given criterion if 'none' is set, no inference is performed here we support only correlation based inference at the moment .overall.crit [ 'none', 'correl', 'correl+simwxy', 'correl+simwx+simwy' ] statistical inference across splits (i.e. 1 permutation test across all data splits) based on given criterion if 'none' is set, no inference is performed here we highlight that for omnibus hypothesis (see Monteiro et al 2016 ) you need to set stat.split.crit = 'correl', stat.overall.crit = 'none' as this approach automatically performs inference across splits using Bonferroni correction for the statistical inference of Mihalik et al 2020 you need to set stat.split.crit = 'none', stat.overall.crit = 'correl' .EBcol [ int or numeric array ] indexes of columns in EB matrix to use for defining exchangeability blocks for restricted permutations data Details of the data and its properties including modailities, dimensionality, block structure. The preprocessing strategy and the filenames with full path are also defined here. We highlight that when the data comes in a preprocessed format (e.g. imputed, z-scored), inference using holdout or CV framework might be invalid (i.e., p-values inflated). We also highly recommend to quality check your data before entering it into the toolkit to remove data with too many missing values, outliers or with highly imbalanced reponses (e.g. using qc_data.m). .block [ boolean ] defines if there is a block structure in the data, i.e. examples are not independent of each other .preproc [ cell array --> {'impute', 'deconf', 'zscore'} ] data preprocessing strategy including missing value imputation, z-scoring and deconfounding of note, data preprocessing is calculated on training data and applied to test data if 'cv' or 'holdout' framework used .mod [ cell array ] data modalaties to be used (e.g. {'X' 'Y'}) .X.fname, .Y.fname, .C.fname [ filepath --> 'X.mat', 'Y.mat', 'C.mat' ] file with full path to data X, Y, C .X.impute, .Y.impute, .C.impute [ 'median' ] strategy to impute missing values if there are missing values in the data, the proportion of missing values is displayed in the command line during data preprocessing .X.nfeat, .Y.nfeat [ int ] number of features/variables in data X, Y .nsubj [ int ] number of subjects/examples env Computation environment can be either local or a cluster. In the latter case, we currently support SGE or SLURM scheduling systems. There is an important technical issue when runnning experiments on a cluster using multiple nodes/jobs. MATLAB is able to load the same file from different nodes but not able to save in parallel from different nodes. We developed a system where each node saves different physical copies of the same file when saving exactly at the same time, however, once a file is saved completely other nodes are able to access it. The different copies of the files are distinguished by the identifier of the nodes which is obtained from the scheduling system, saved in 'cfg.env.fileend' then appended to the end of the name of the file when saving. .comp [ 'local', 'cluster' ] computation environment .commit [ char ] SHA hash of the latest commit in git (i.e., toolkit vesion) for reproducibility .OS [ 'mac', 'unix', 'pc' ] operating system (OS) this information is used when transferring files between OS and updating paths (see cfg.dir fields and update_dir.m) .fileend [ char --> '_1' ] suffix at the end of each file saved in the framework folder whilst running the experiment on a cluster for file storage efficiency and easier data transfer, we suggest to use 'cleanup_files.m' after an experiment is completed which deletes the unnecessary copies of the same file and replaces the 'fileend' to the default value of '_1' .save.compression [ boolean --> 1 ] defines if files are saved with or without compression of note, loading an uncompressed file can be faster for very large data files See also: res_defaults Author: Agoston Mihalik Website: MLNL","title":"Cfg Defaults"},{"location":"mfiles/cfg_defaults/#syntax","text":"cfg = cfg_defaults(cfg, varargin)","title":"Syntax:"},{"location":"mfiles/cfg_defaults/#inputs","text":"cfg [ cfg structure ] varargin [ name-value pairs ] additional parameters can be set via name-value pairs with dot notation supported (e.g., 'frwork.split.nout', 5)","title":"Inputs"},{"location":"mfiles/cfg_defaults/#outputs","text":"cfg [ cfg structure ] configuration structure that has been updated with defaults","title":"Outputs"},{"location":"mfiles/cfg_defaults/#examples","text":"% Example 1 cfg . dir . project = 'PATH/TO/YOUR/PROJECT1' cfg = cfg_defaults ( cfg ) % Example 2 cfg . dir . project = 'PATH/TO/YOUR/PROJECT2' cfg = cfg_defaults ( cfg , 'machine.name' , 'spls' , ... 'machine.metric' , { 'correl' 'simwx' 'simwy' }, 'machine.param.crit' , 'correl+simwxy' ) % Example 3 cfg = cfg_defaults ([], 'dir.project' , 'PATH/TO/YOUR/PROJECT3' , ... 'machine.name' , 'rcca' , ... 'machine.metric' , { 'correl' }, 'machine.param.crit' , 'correl' , 'frwork.split.nout' , 5 )","title":"Examples"},{"location":"mfiles/cfg_defaults/#fields-and-options","text":"You can find a description of all possible settings of the fields and subfields of cfg below. First parameter always indicates the default option.","title":"Fields and options"},{"location":"mfiles/cfg_defaults/#dir","text":"Essential paths to your project, framework and processed data. Include 'simulation' in your project folder if you want to generate simulated data using Uurtio's github repo . The project folder should include a 'data' folder where all the input data are stored. For running experiments, you need X.mat , Y.mat , and optionally C.mat , EB.mat . For plotting results, you need label files in a tabulated text format (e.g., LabelsX.xlsx , LabelsY.xlsx ), a mask file (e.g. mask.mat or mask.nii ) for VBM or connectivity data. Depending on your plots you might need additional files too. For details, see res_defaults . .project [ path ] full path to your project, such as 'PATH/TO/YOUR/PROJECT' .frwork [ path ] full path to your framework, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME' analysis name is generated from machine name and framework settings (including flag) .load [ path ] full path to your processed data, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/load' it stores information about data processing for computational efficiency including preprocessing (e.g. mean and scale of features, betas for deconfounding) and results of PCA if CCA/RCCA is used","title":"dir"},{"location":"mfiles/cfg_defaults/#machine","text":"Algorithm to be used, its settings and information about hyperparameter optimization. Please make sure that you are familiar with the hyperparameter settings of the chosen algorithm, e.g. range and scale of hyperparameter values for grid search or number of PCA components. In the latter case, if not data driven PCA components are chosen, we highly recommend to inspect your data before chosing the right amount of PCA components (e.g. using assess_pca_comp.m). We also strongly encourage to use RCCA or SPLS as CCA and PLS are likely to overfit the data and/or loose sensitivity unless the number of examples greatly exceed the number of features. .name [ 'cca', 'rcca', 'pls', 'spls' ] name of the algorithm PCA is implicitly used in CCA/RCCA algorithms, so set machine name to CCA if you want to perform PCA-CCA analysis RCCA smooths between CCA and PLS defined by the L2 regularization hyperparameter (L2=0 for CCA, L2=1 for PLS) .metric [ cell array ] metrics that are used to evaluate the machine including 'correl' (test correlation), 'covar' (test covariance), 'simwx' (similarity of X weights), 'simwy' (similarity of Y weights), 'unsuc' (number of unsuccessful convergence for SPLS) .param.crit [ 'correl', 'correl+simwxy', 'correl+simwx+simwy', 'covar', 'freq' ] criterion for hyperparameter selection, we recommend using 'correl' or 'correl+simwxy' 'correl+simwxy' uses a 2D distance metric based on correlation and averaged similarity of weights 'correl+simwx+simwy' uses a 3D distance metric based on correlation and similarity of X weights and Y weights .param.name [ cell array ] name of the hyperparameters, including 'L1x' and 'L1y' for L1 regularization (for sparsity), 'L2x' and 'L2y' for L2 regularization (for stability), 'PCAx' and 'PCAy' for number of PCA components we note that L2 regularization can be used together with PCA .param.type [ 'factorial', 'matched' ] defines if grid search for hyperparameters should be a factorial combination of all hyperparameters or matching correspoding hyperparameters (in this case all should have the same length) .param.L1x, .param.L1y [ int or numeric array ] amount of L1 regularization for data X, Y when using SPLS if not provided, the function generates a linearly scaled numeric array using the number and range of hyperparameters (see below) .param.nL1x, .param.nL1y [ int --> 10 ] number of L1 regularization hyperparameters for data X, Y used in grid search .param.rangeL1x, .param.rangeL1y [ numeric array ] range of L1 regularization hyperparameters for data X, Y used in grid search for L1 regularization to be active, range should be between 1 and square root of the number of features, otherwise PLS is used .param.L2x, .param.L2y [ int ] amount of L2 regularization for data X, Y when using RCCA if set to 0 RCCA performs CCA, if set to 1 RCCA performs PLS use 1 - inverse of the number of features for a decent amount of regularization if not provided, the function generates a logarithmically scaled numeric array using the number and range of hyperparameters (see below) .param.nL2x, .param.nL2y [ int ] number of L2 regularization hyperparameters for data X, Y used in grid search .param.rangeL2x, .param.rangeL2y [ numeric array --> [0 1] ] range of L2 regularization hyperparameters for data X used in grid search .param.PCAx, .param.PCAy [ int ] number of PCA components for data X, Y if not provided, the function uses .eig.tol to deal with rank-deficiency .param.VARx, .param.VARy [ int --> 0.99 ] retained variance during PCA step of CCA/RCCA .eig.tol [ int --> 1e-10 ] eigenvalues smaller than tolerance are removed during PCA step of CCA/RCCA .conv.tol [ int --> 1e-5; ] tolerance during SPLS convergence .conv.maxiter [ int --> 100 ] maximum number of iterations during SPLS convergence","title":"machine"},{"location":"mfiles/cfg_defaults/#frwork","text":"Details of framework with three main approaches. Divide data to training and test sets using multiple holdouts (see Monteiro et al 2016 ) by randomly subsampling subjects or standard cross-validation by applying folds. Otherwise, use a permutation approach without any data splitting (see Smith et al 2015 ). The default values will change depending on the type of the framework. .name [ 'holdout', 'cv', 'permutation' ] type of the framework .flag [ char ] a short name/flag to be appended to your analysis name which will then define the framework folder, see cfg.dir.frwork .split.nout [ int ] number of outer splits/folds .split.propout [ float --> 0.2 ] proportion of holdout/test set in 'holdout' framework higher value is recommended for samples n<500 (e.g. 0.2-0.5), and lower value (e.g. 0.1) should be sufficient for samples n > 1000 set to 0 for 'permutation' framework .split.nin [ int ] number of inner splits/folds .split.propin [ float --> 0.2 ] proportion of validation set in 'holdout' framework .split.EBcol [ int or numeric array ] indexes of columns in EB matrix to use for defining exchangeability blocks for data partitioning if multi-level blocks are provided, most likely you need to provide 2 columns here as e.g. no cross-over across different family types (column 2) but shuffling across families (column 3) within same family type are allowed, in other words, families should be in the same data split (training or test)","title":"frwork"},{"location":"mfiles/cfg_defaults/#defl","text":"Deflation types and strategies. For an in-depth introduction to deflation (i.e. iterative solution of CCA/PLS) and its different types, see Home . As most of the time we use different outer splits/folds of the data, it is of interest which split to use as the basis for deflation. A natural choice is to use the weights of the best data split (e.g. with highest holdout correlation and similarity across weights) and deflate all other splits with it. Other options are e.g. to deflate significant splits either by using the weights of the best split or use all splits independently. We note, that only the latest approach is in its pure sense a matrix decomposition (especially when standard CCA/PLS or the same regularization hyperparameter is used across associative effects) as in the other cases we mix effects across splits. Nevertheless, in practice we can gain robustness of the results by choosing e.g. best data split and/or deflating all data splits. .name [ 'cca-projection', 'pls-projection', 'pls-modeA', 'pls-regression' ] type of deflation .crit [ 'correl', 'pval+correl', 'correl+simwxy', 'correl+simwx+simwy', 'none' ] criterion to define best (i.e. most representative) data split to be used for deflation if 'none' set then each split is deflated by itself (i.e. they are treated independently) .split [ 'significant', 'all' ] selection of splits for deflation, i.e. whether all splits are deflated or only the significant ones (the latter approach needs significance testing within splits)","title":"defl"},{"location":"mfiles/cfg_defaults/#stat","text":"Statistical inference. Our approach is to make inference about the multivariate associative effect and not necessarily about specific model weights unless sparsity is used to select features/variables. Testing the generalizability of the models (i.e., using test correlations) is one of our key recommendations. Furthermore, we advise to check the robustness (i.e., how many data splits are significant) and the stability (i.e., similarity of the weights) of the models and not just rely purely on p-values. There are two main statistical inferences in the toolkit. One is based on an omnibus hypothesis proposed by Monteiro et al 2016 which performs statistical inference both within and across splits. The other is based on statistical inference across splits and relies highly on the stability of models across outer splits (see Mihalik et al 2020 ). The first approach is more conservative and might produce false negative results (especially in datasets with <500 subjects), whilst the latter might be too permissive when working with large datasets (>1000 subjects). .nperm [ int ] number of permutations .split.crit [ 'correl', 'none' ] statistical inference within splits (i.e., 1 permutation test for each data split) based on given criterion if 'none' is set, no inference is performed here we support only correlation based inference at the moment .overall.crit [ 'none', 'correl', 'correl+simwxy', 'correl+simwx+simwy' ] statistical inference across splits (i.e. 1 permutation test across all data splits) based on given criterion if 'none' is set, no inference is performed here we highlight that for omnibus hypothesis (see Monteiro et al 2016 ) you need to set stat.split.crit = 'correl', stat.overall.crit = 'none' as this approach automatically performs inference across splits using Bonferroni correction for the statistical inference of Mihalik et al 2020 you need to set stat.split.crit = 'none', stat.overall.crit = 'correl' .EBcol [ int or numeric array ] indexes of columns in EB matrix to use for defining exchangeability blocks for restricted permutations","title":"stat"},{"location":"mfiles/cfg_defaults/#data","text":"Details of the data and its properties including modailities, dimensionality, block structure. The preprocessing strategy and the filenames with full path are also defined here. We highlight that when the data comes in a preprocessed format (e.g. imputed, z-scored), inference using holdout or CV framework might be invalid (i.e., p-values inflated). We also highly recommend to quality check your data before entering it into the toolkit to remove data with too many missing values, outliers or with highly imbalanced reponses (e.g. using qc_data.m). .block [ boolean ] defines if there is a block structure in the data, i.e. examples are not independent of each other .preproc [ cell array --> {'impute', 'deconf', 'zscore'} ] data preprocessing strategy including missing value imputation, z-scoring and deconfounding of note, data preprocessing is calculated on training data and applied to test data if 'cv' or 'holdout' framework used .mod [ cell array ] data modalaties to be used (e.g. {'X' 'Y'}) .X.fname, .Y.fname, .C.fname [ filepath --> 'X.mat', 'Y.mat', 'C.mat' ] file with full path to data X, Y, C .X.impute, .Y.impute, .C.impute [ 'median' ] strategy to impute missing values if there are missing values in the data, the proportion of missing values is displayed in the command line during data preprocessing .X.nfeat, .Y.nfeat [ int ] number of features/variables in data X, Y .nsubj [ int ] number of subjects/examples","title":"data"},{"location":"mfiles/cfg_defaults/#env","text":"Computation environment can be either local or a cluster. In the latter case, we currently support SGE or SLURM scheduling systems. There is an important technical issue when runnning experiments on a cluster using multiple nodes/jobs. MATLAB is able to load the same file from different nodes but not able to save in parallel from different nodes. We developed a system where each node saves different physical copies of the same file when saving exactly at the same time, however, once a file is saved completely other nodes are able to access it. The different copies of the files are distinguished by the identifier of the nodes which is obtained from the scheduling system, saved in 'cfg.env.fileend' then appended to the end of the name of the file when saving. .comp [ 'local', 'cluster' ] computation environment .commit [ char ] SHA hash of the latest commit in git (i.e., toolkit vesion) for reproducibility .OS [ 'mac', 'unix', 'pc' ] operating system (OS) this information is used when transferring files between OS and updating paths (see cfg.dir fields and update_dir.m) .fileend [ char --> '_1' ] suffix at the end of each file saved in the framework folder whilst running the experiment on a cluster for file storage efficiency and easier data transfer, we suggest to use 'cleanup_files.m' after an experiment is completed which deletes the unnecessary copies of the same file and replaces the 'fileend' to the default value of '_1' .save.compression [ boolean --> 1 ] defines if files are saved with or without compression of note, loading an uncompressed file can be faster for very large data files See also: res_defaults Author: Agoston Mihalik Website: MLNL","title":"env"},{"location":"mfiles/plot_paropt/","text":"plot_paropt Syntax: plot_paropt(res, mod, split, varargin) Inputs res [ struct ] res structure containing information about results and plot specifications mod [ cell array ] modality of data to be used for plotting (i.e., {'X', 'Y'}) split [ int ] index of data split to be used varargin [ 'correl', 'covar', 'simwx', 'simwy', 'simwxy', 'correl+simwxy' ] metrics to be plotted as a function of hyperparameter grid, each metric in a separate subplot Example % Plot hyperparameter surface for grid search results plot_paropt ( res , { 'X' 'Y' }, res . frwork . split . best , 'correl' , 'simwxy' , ... 'correl+simwxy' ); See also: plot_proj , plot_weight Author: Agoston Mihalik Website: MLNL","title":"plot_paropt"},{"location":"mfiles/plot_paropt/#syntax","text":"plot_paropt(res, mod, split, varargin)","title":"Syntax:"},{"location":"mfiles/plot_paropt/#inputs","text":"res [ struct ] res structure containing information about results and plot specifications mod [ cell array ] modality of data to be used for plotting (i.e., {'X', 'Y'}) split [ int ] index of data split to be used varargin [ 'correl', 'covar', 'simwx', 'simwy', 'simwxy', 'correl+simwxy' ] metrics to be plotted as a function of hyperparameter grid, each metric in a separate subplot","title":"Inputs"},{"location":"mfiles/plot_paropt/#example","text":"% Plot hyperparameter surface for grid search results plot_paropt ( res , { 'X' 'Y' }, res . frwork . split . best , 'correl' , 'simwxy' , ... 'correl+simwxy' ); See also: plot_proj , plot_weight Author: Agoston Mihalik Website: MLNL","title":"Example"},{"location":"mfiles/plot_proj/","text":"plot_proj Syntax: plot_proj(res, mod, level, sidvar, split, label, func, varargin) Inputs res [ struct ] res structure containing information about results and plot specifications mod [ cell array ] modality of data to be used for plotting (i.e., {'X', 'Y'}) level [ int or numeric array ] level of associative effect with same dimensionality as 'mod' or automatically extended (e.g. from int to numeric array) sidvar [ 'osplit', 'otrid', 'oteid', 'isplit', 'itrid', 'iteid' ] specifies subjects to be used for plotting first letter can be 'o' for outer or 'i' for inner split, followed by either 'trid' for training, 'teid' for test or 'split' for both training and test data split [ int or numeric array ] index of data split to be used with same dimensionality as 'mod' or automatically extended (e.g. from int to numeric array) label [ 'none', char ] 'none' for scatterplot with same colour for all subjects or label (e.g. from LabelsY.xlsx) to be used as a continuous colormap (e.g. Age) or for colouring different groups (e.g. Male); label file and corresponding data file are specified by 'res.proj.file.label' and 'res.proj.file.data' if '+' is included in the character (e.g. 'MDD+HC') group information is taken from cfg.data.group func [ '2d', '2d_group', '2d_cmap' ] name of the specific plotting function (after plot_proj_* prefix) to be called varargin [ name-value pairs ] additional options can be passed via name-value pairs with dot notation supported (e.g., 'proj.xlim', [-5 5]) Examples For more examples see plotting templates. Simple Plots Most often, we plot brain score vs. behaviour score for a specific level (i.e., associative effect). % Plot data projections coloured by groups provided in data/label files res . proj . file . data = fullfile ( res . dir . project , 'data' , 'V.mat' ); res . proj . file . label = fullfile ( res . dir . project , 'data' , 'LabelsV.xlsx' ); plot_proj ( res , { 'X' 'Y' }, res . frwork . level , 'osplit' , res . frwork . split . best , ... 'Remission' , '2d_group' ); Multi Level Plots To plot projections aggregated over multiple levels, all you need to specify is res.proj.multi_level = 1 and provide a 2D cell array of input variable 'mod'. Input variables 'level' and 'split' should have the same dimensionality or they will be extended automatically from 1-D or 2-D arrays (e.g. level = repmat(level, size(mod))). % Plot data projections across levels (and averaged over modalities % in a given level after standardization) res . proj . multi_label = 1 ; plot_proj ( res , { 'X' 'Y' ; 'X' 'Y' }, [ 1 1 ; 2 2], 'osplit', res.frwork.split.best, ... 'Remission' , '2d_group' ); See also: plot_paropt , plot_weight Author: Agoston Mihalik Website: MLNL","title":"plot_proj"},{"location":"mfiles/plot_proj/#syntax","text":"plot_proj(res, mod, level, sidvar, split, label, func, varargin)","title":"Syntax:"},{"location":"mfiles/plot_proj/#inputs","text":"res [ struct ] res structure containing information about results and plot specifications mod [ cell array ] modality of data to be used for plotting (i.e., {'X', 'Y'}) level [ int or numeric array ] level of associative effect with same dimensionality as 'mod' or automatically extended (e.g. from int to numeric array) sidvar [ 'osplit', 'otrid', 'oteid', 'isplit', 'itrid', 'iteid' ] specifies subjects to be used for plotting first letter can be 'o' for outer or 'i' for inner split, followed by either 'trid' for training, 'teid' for test or 'split' for both training and test data split [ int or numeric array ] index of data split to be used with same dimensionality as 'mod' or automatically extended (e.g. from int to numeric array) label [ 'none', char ] 'none' for scatterplot with same colour for all subjects or label (e.g. from LabelsY.xlsx) to be used as a continuous colormap (e.g. Age) or for colouring different groups (e.g. Male); label file and corresponding data file are specified by 'res.proj.file.label' and 'res.proj.file.data' if '+' is included in the character (e.g. 'MDD+HC') group information is taken from cfg.data.group func [ '2d', '2d_group', '2d_cmap' ] name of the specific plotting function (after plot_proj_* prefix) to be called varargin [ name-value pairs ] additional options can be passed via name-value pairs with dot notation supported (e.g., 'proj.xlim', [-5 5])","title":"Inputs"},{"location":"mfiles/plot_proj/#examples","text":"For more examples see plotting templates.","title":"Examples"},{"location":"mfiles/plot_proj/#simple-plots","text":"Most often, we plot brain score vs. behaviour score for a specific level (i.e., associative effect). % Plot data projections coloured by groups provided in data/label files res . proj . file . data = fullfile ( res . dir . project , 'data' , 'V.mat' ); res . proj . file . label = fullfile ( res . dir . project , 'data' , 'LabelsV.xlsx' ); plot_proj ( res , { 'X' 'Y' }, res . frwork . level , 'osplit' , res . frwork . split . best , ... 'Remission' , '2d_group' );","title":"Simple Plots"},{"location":"mfiles/plot_proj/#multi-level-plots","text":"To plot projections aggregated over multiple levels, all you need to specify is res.proj.multi_level = 1 and provide a 2D cell array of input variable 'mod'. Input variables 'level' and 'split' should have the same dimensionality or they will be extended automatically from 1-D or 2-D arrays (e.g. level = repmat(level, size(mod))). % Plot data projections across levels (and averaged over modalities % in a given level after standardization) res . proj . multi_label = 1 ; plot_proj ( res , { 'X' 'Y' ; 'X' 'Y' }, [ 1 1 ; 2 2], 'osplit', res.frwork.split.best, ... 'Remission' , '2d_group' ); See also: plot_paropt , plot_weight Author: Agoston Mihalik Website: MLNL","title":"Multi Level Plots"},{"location":"mfiles/plot_weight/","text":"plot_weight Syntax: plot_weight(res, mod, modtype, split, func, varargin) Inputs res [ struct ] res structure containing information about results and plot specifications mod [ 'X', 'Y' ] modality of data to be used for plotting modtype [ 'behav', 'conn', 'vbm', 'roi', 'simul', 'brainnet' ] type of data split [ int ] index of data split to be used func [ 'behav_horz', 'behav_vert', 'brain_conn_node', 'brain_cortex', 'brain_edge', 'brain_module', 'brain_node', 'brain_schemaball', 'brain_subcortex', 'stem' ] name of the specific plotting function (after plot_weight_* prefix) to be called varargin [ name-value pairs ] additional options can be passed via name-value pairs with dot notation supported (e.g., 'behav.weight.numtop', 20) Examples For more examples see plotting templates. Behavioural % Plot top behavioural weights as horizontal bar plot res . behav . weight . sorttype = 'sign' ; res . behav . weight . numtop = 20 ; res . behav . label . maxchar = 50 ; plot_weight ( res , 'Y' , 'behav' , res . frwork . split . best , 'behav_horz' ); Connectivity % Plot top connectivity weights on glass brain res . conn . file . label = fullfile ( res . dir . project , 'data' , ... 'Schaefer2018_400Parcels_7Networks_order_FSLMNI152_1mm.csv' ); res . conn . weight . sorttype = 'sign' ; res . conn . weight . numtop = 20 ; res . brainnet . file . surf = 'PATH/TO/BrainMesh_ICBM152_smoothed.nv' ; res . brainnet . file . options = fullfile ( res . dir . project , 'data' , 'BrainNet' , ... 'options_edge.mat' ); plot_weight ( res , 'X' , 'conn' , res . frwork . split . best , 'brain_edge' ); % Plot top connectivity weights as schemaball plot_weight ( res , 'X' , 'conn' , res . frwork . split . best , 'brain_schemaball' ); See also: plot_paropt , plot_proj Author: Agoston Mihalik Website: MLNL","title":"plot_weight"},{"location":"mfiles/plot_weight/#syntax","text":"plot_weight(res, mod, modtype, split, func, varargin)","title":"Syntax:"},{"location":"mfiles/plot_weight/#inputs","text":"res [ struct ] res structure containing information about results and plot specifications mod [ 'X', 'Y' ] modality of data to be used for plotting modtype [ 'behav', 'conn', 'vbm', 'roi', 'simul', 'brainnet' ] type of data split [ int ] index of data split to be used func [ 'behav_horz', 'behav_vert', 'brain_conn_node', 'brain_cortex', 'brain_edge', 'brain_module', 'brain_node', 'brain_schemaball', 'brain_subcortex', 'stem' ] name of the specific plotting function (after plot_weight_* prefix) to be called varargin [ name-value pairs ] additional options can be passed via name-value pairs with dot notation supported (e.g., 'behav.weight.numtop', 20)","title":"Inputs"},{"location":"mfiles/plot_weight/#examples","text":"For more examples see plotting templates.","title":"Examples"},{"location":"mfiles/plot_weight/#behavioural","text":"% Plot top behavioural weights as horizontal bar plot res . behav . weight . sorttype = 'sign' ; res . behav . weight . numtop = 20 ; res . behav . label . maxchar = 50 ; plot_weight ( res , 'Y' , 'behav' , res . frwork . split . best , 'behav_horz' );","title":"Behavioural"},{"location":"mfiles/plot_weight/#connectivity","text":"% Plot top connectivity weights on glass brain res . conn . file . label = fullfile ( res . dir . project , 'data' , ... 'Schaefer2018_400Parcels_7Networks_order_FSLMNI152_1mm.csv' ); res . conn . weight . sorttype = 'sign' ; res . conn . weight . numtop = 20 ; res . brainnet . file . surf = 'PATH/TO/BrainMesh_ICBM152_smoothed.nv' ; res . brainnet . file . options = fullfile ( res . dir . project , 'data' , 'BrainNet' , ... 'options_edge.mat' ); plot_weight ( res , 'X' , 'conn' , res . frwork . split . best , 'brain_edge' ); % Plot top connectivity weights as schemaball plot_weight ( res , 'X' , 'conn' , res . frwork . split . best , 'brain_schemaball' ); See also: plot_paropt , plot_proj Author: Agoston Mihalik Website: MLNL","title":"Connectivity"},{"location":"mfiles/res_defaults/","text":"res_defaults Set defaults in your results ( res ) structure including information about the results and settings for plotting. Use this function to update and add all necessary defaults to your res . If you have defined anything in res before calling the function, it won't overwrite those values. The path to the framework folder should be always defined in your res or passed as varargin, otherwise the function throws an error. All the other fields are optional and can be filled up by res_defaults . This function can be also called to load an existing res.mat file. Warning We strongly advise to inspect the output of res_defaults to make sure that the defaults are set as expected. Syntax: res = res_defaults(res, mode, varargin) Inputs res [ struct ] results structure (more information below) mode [ 'init', 'load', 'projection', 'simul', 'behav', 'conn', 'vbm', 'roi', 'brainnet' ] mode of calling res_defaults, either referring to initialization ('init'), loading ('load'), type of plot ('projection', 'simul', 'behav', 'conn', 'vbm', 'roi') or settings for toolbox ('brainnet') varargin [ name-value pairs ] additional parameters can be set via name-value pairs with dot notation supported (e.g., 'behav.weight.numtop', 20) Outputs res [ struct ] result structure that has been updated with defaults Examples % Example 1 res . dir . frwork = 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME' ; res . frwork . level = 1 ; res . env . fileend = '_1' ; res = res_defaults ( res , 'load' ); % Example 2 res = res_defaults ([], 'load' , 'dir.frwork' , ... 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME' ); % Example 3 res = res_defaults ([], 'load' , 'dir.frwork' , ... 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME' ); res = res_defaults ( res , 'behav' ); Fields from results You can find a description of the fields and subfields of res obtained during the experiment. For more transparency, all the root fields are inherited from cfg . dir Essential paths to your project, framework (subfields inherited from cfg , see here ) and the output folders of the experiment such as grid search, permutations and main results. The results folder by default contains a summary results table and three mat files, one for the res file, one for the hyperparameters used for the models on the training set (or combined training and validation set when using grid search), and one for the outputs of these models (including e.g. weights and holdout/test correlations). .project [ path ] full path to your project, such as 'PATH/TO/YOUR/PROJECT' .frwork [ path ] full path to your framework, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME' .grid [ path ] full path to your grid search results, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/grid/level ' .perm [ path ] full path to your permutation testing results, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/perm/level ' .res [ path ] full path to your main results, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/grid/level ' frwork Results-orinted details of framework with different subfields as in cfg . .level [ int ] level of the multivariate associative effect in the iterative calculation process (often called 'mode' in the CCA literature) .split.all [ int or numeric array ] all splits at the current level .split.nall [ int ] number of all splits at the current level .split.sig [ int or numeric array ] all splits at the current level that passed significance .split.best best split based on the criterion defined by cfg.defl.crit (for details, see here ) stat Results of statistical inference with some subfields inherited from cfg . For further details on the type of statistical inferences, see the description here . .nperm [ int ] number of permutations .split.pval [ int or numeric array ] uncorrected p-values of the statistical inference within splits (one p-value per split) of note, in case of 'omnibus' hypothesis the associative effect is significant if any of the p-values are smaller than 0.05 / number of splits .overall.pval [ int ] p-value of the statistical inference across splits of note, this is not provided for the statistical inference with 'omnibus' hypothesis as that requires only a Bonferroni correction but no p-values are calculated across splits env Computation environment with the 'fileend' and 'save' subfields inherited from cfg . For further details see here . .fileend [ char --> '_1' ] suffix at the end of each file saved in the framework folder whilst running the experiment on a cluster for file storage efficiency and easier data transfer, we suggest to use 'cleanup_files.m' after an experiment is completed which deletes the unnecessary copies of the same file and replaces the 'fileend' to the default value of '_1' .save.compression [ boolean --> 0 ] defines if files are saved with or without compression of note, loading an uncompressed file can be faster for very large data files Fields and options for plotting You can find a description of all possible settings of the fields and subfields of res for plotting below. First parameter always indicates the default option. gen General options for plotting. We recommend using 'interactive' file selection when new to plotting results to be able to understand what files are needed for the specific plots. Later, especially when wishing to automatize plots, it might be convenient to use 'none' file selection to avoid the interactive pop-up window and the files can be easily controlled/provided by setting the appropriate fields (e.g. 'file.label'). .selectfile [ 'none', 'interactive' ] file selection using a wrapper function over the select_file function of SPM .weight.flip [ boolean --> false ] choice of flipping the weights, i.e. changing their sign .weight.type [ 'weight', 'correlation' ] type/interpretation of weight, i.e. how much each feature/variable contributes to the associative effect 'weight' refers to the model weights read from model.mat , which is most often used in the PLS literature 'correlation' refers to the correlation between the input features/variables and the latent variables/projections (i.e., brain and behavioural scores), which is most often used in the CCA literature data Subfields of 'fname' inherited from cfg . For further details see here . .X.fname, .Y.fname, .C.fname [ filepath --> 'X.mat', 'Y.mat', 'C.mat' ] file with full path to data X, Y, C param Options for plotting hyperparameter optimization results from grid search. For the plotting function, see plot_paropt . .xscale, .yscale [ 'lin', 'log' ] scale for x, y axis for RCCA, a logarithmic scale is recommended for SPLS, a linear scale is recommended proj Options for plotting projections of data (i.e. scores/latent variables). For the plotting function, see plot_proj . In this scatter plot each dot is a subject and as the axes are latent variables, it is also called as latent space plot. Most often, we plot brain score vs. behaviour score for a specific level (i.e., associative effect). However, it is possible to plot the latent space across multiple levels (and averaged over modalities in a given level after standardization). The latent space can be colour-coded by a continuous variable used as a colormap or a discrete variable with different colours for the different groups of data. In this case, a label file and a corresponding data file should be provided. .xlabel, .ylabel [ char --> 'Brain score', 'Behavioural score' ] label for x, y axis .xlim, y.lim [ numeric array --> NaN ] limit for x, y axis similar to MATLAB's built-in 'xlim', 'ylim' .file.label [ filepath --> 'LabelsY.xlsx' ] label file with full path for additional colormap/group information label file and data file (see below) should be corresponding to each other, i.e. row i in label file (without header) corresponds to column i in data file .file.data [ filepath --> 'Y.mat'; ] data file with full path for additional colormap/group information label file and data file (see above) should be corresponding to each other, i.e. row i in label file (without header) corresponds to column i in data file .flip [ boolean --> false ] choice of flipping the weights, i.e. changing their sign .multi_level [ boolean --> 0 ] choice for multi-level projection plots, i.e. averaging across modalities behav Weight postprocessing options, label file and figure settings for plotting weights of labelled behavioural data as horizontal or vertical bar plots. For the general plotting function, see plot_weight . .weight.filtzero [ boolean --> 1 ] postprocess weights removing weights with zero values .weight.numtop [ int --> Inf ] postprocess weights by number of top weights 'Inf' refers to including all weights .weight.sorttype [ 'sign', ' ', 'abs' ] postprocess weights by sorting them in descending order 'sign' sorts both positive and negative weights in descending order (i.e., as if they were two independent lists) 'abs' sorts weights based on absolute value '' refers to no sorting .weight.cutlabels [ boolean --> 1 ] shorten labels that are too long .file.label [ filepath --> 'LabelsY.xlsx' ] label file with full path for Y data .label.maxchar [ int --> Inf ] maximum number of characters for label names in figure of note, use it when some of the labels are too long to display on the figure setting it to e.g. 50 conn Options for plotting connectivity data e.g. from resting-state fMRI. For the general plotting function, see plot_weight and for a template plotting connectivity data, see here . As brain data is in concatenated format (subject x feature matrix) and it might not include all pairwise connections, a mask file is necessary to be able to reshape the data to node x node format. A node can be an anatomical or functional Region of Interest (ROI) or an Independent Component Analyis (ICA) component. In the previous cases we recommend using BrainNet Viewer to plot weights on a glass brain as edges or summarized as nodes (for settings, see here ). In the latter case, we recommend using a schemaball plot (see plot_weight_brain_schemaball.m). It is also possible to summarize the weights as connections within/between modules and we use MATLAB's built-in imagesc function to plot the results. .file.mask [ filepath --> 'mask.mat' ] mask file with full path for connectivity data 'mask' variable is a matrix with booleans for the connections that are selected for brain data after concatenation (e.g. lower triangular part) .weight.filtzero [ boolean --> 1 ] postprocess weights removing weights with zero values .weight.numtop [ int --> Inf ] postprocess weights by number of top weights 'Inf' refers to including all weights .weight.sorttype [ 'sign', ' ', 'abs' ] postprocess weights by sorting them in descending order 'sign' sorts both positive and negative weights in descending order (i.e., as if they were two independent lists) 'abs' sorts weights based on absolute value '' refers to no sorting .weight.type [ 'auto', 'strength' ] postprocess weights by multiplying them by the sign of the population mean in the original data 'auto' does no postprocessing 'strength' does postprocessing .weight.sign [ 'all', 'positive', 'negative' ] postprocess weights by selecting a subset of them (e.g. with positive or negative sign) 'all' does no postprocessing .module.disp [ boolean --> 0 ] choice to display module weights in command line .module.type [ 'average', 'sum' ] calculate the average or sum of weights within/between modules .module.norm [ 'none', 'global', 'max' ] normalize module weights... 'none' does no normalization .file.label [ filepath --> 'LabelsX.xlsx' ] label file with full path for connectivity data vbm Options for plotting Voxel Based Morphometry (VBM), i.e. voxel-wise data. For the general plotting function, see plot_weight and for a template plotting VBM data, see here . As brain data is in concatenated format (subject x feature matrix), a mask file is necessary to be able to write the weights in nii format. By default, the brain weights are assumed to be in MNI space, however, normalization is possible if given a template/source image and a rigid body transformation matrix to reorient the image to approximately match the MNI template space (otherwise, if the weight image is way out of MNI space, the normalization fails). We use BrainNet Viewer to plot the cortical weights on a glass brain (for settings, see here ). It is also possible to plot the brain weights separately for cortex and subcortex, in which case a nii atlas with a corresponding label file (in tabulated format with a column defining cortex/subcortex) is needed (see e.g. AAL ) to be able to obtain information about the cortical/subcortical position of each voxel. For plotting subcortical regions, Nilearn should be installed on the machine. .file.mask [ filepath --> 'mask.nii' ] mask file with full path for VBM data nii file includes an image with booleans for the voxels that are selected for brain data after concatenation .file.MNI [ filepath --> 'T1_1mm_brain.nii' ] template/source image with full path for normalization .transM [ numeric array --> eye(4) ] rigid body transformation matrix to reorient the weight image to MNI space before normalization occurs of note, for the current ABCD analyses it is [1 0 0 118.6; 0 1 0 -128.6; 0 0 1 -63.3; 0 0 0 1] .file.atlas.img [ filepath --> 'AAL2.nii' ] atlas nii file in MNI space for definition of cortex/subcortex of note, full path is not necessary if file is in path or 'aal' folder in the external folder .file.atlas.label [ filepath --> 'Labels_AAL2.xlsx' ] atlas label file for for definition of cortex/subcortex of note, full path is not necessary if file is in path or 'aal' folder in the external folder .subcortex [ boolean --> 0 ] choice to plot subcortical volume separately and remove it from cortical plot .file.nilearn.MNI [ filepath --> 'MNI152_T1_1mm_brain.nii' ] MNI template file with full path for background image in Nilearn of note, only relevant if 'res.vbm.subcortex' is 1 roi Weight postprocessing options and label file for plotting weights of Region Of Interest (ROI) data. For the general plotting function, see plot_weight and for a template plotting ROI data, see here . The settings here refer to genuine ROI data entered into the experiment. Post-hoc ROI summarization will be added as a feature to VBM data later. .weight.filtzero [ boolean --> 1 ] postprocess weights removing weights with zero values .weight.numtop [ int --> Inf ] postprocess weights by number of top weights 'Inf' refers to including all weights .weight.sorttype [ 'sign', ' ', 'abs' ] postprocess weights by sorting them in descending order 'sign' sorts both positive and negative weights in descending order (i.e., as if they were two independent lists) 'abs' sorts weights based on absolute value '' refers to no sorting .file.label [ filepath --> 'LabelsX.xlsx' ] label file with full path for ROI data simul Weight postprocessing options for plotting simulation results. For the general plotting function, see plot_weight and for an example plot, see here . .weight.filtzero [ boolean --> 0 ] postprocess weights removing weights with zero values .weight.numtop [ int --> Inf ] postprocess weights by number of top weights 'Inf' refers to including all weights .weight.sorttype [ ' ', 'sign', 'abs' ] postprocess weights by sorting them in descending order '' refers to no sorting 'sign' sorts both positive and negative weights in descending order (i.e., as if they were two independent lists) 'abs' sorts weights based on absolute value brainnet Settings for BrainNet Viewer for automatic plotting of brain weights on a glass brain and saving it as bitmap image in file. Make sure that your data and the glass brain are in the same space, by default BrainNet Viewer uses MNI space. The options file includes configurations for BrainNet Viewer to use your preferred styling. In case such file does not exist BrainNet Viewer uses its default settings. The BrainNet Viewer GUI is not closed after the visualization is completed, so at first usage you can manually edit your preferred settings and save them as options file for future usage. .file.surf [ filepath --> 'BrainMesh_ICBM152.nv' ] brain mesh (glass brain) file with full path in external folder of note, full path is not necessary if file is in path or 'brainnet' folder in external folder .file.options [ filepath --> 'options.mat' ] options file with full path for BrainNet configuration See also: cfg_defaults Author: Agoston Mihalik Website: MLNL","title":"Res Defaults"},{"location":"mfiles/res_defaults/#syntax","text":"res = res_defaults(res, mode, varargin)","title":"Syntax:"},{"location":"mfiles/res_defaults/#inputs","text":"res [ struct ] results structure (more information below) mode [ 'init', 'load', 'projection', 'simul', 'behav', 'conn', 'vbm', 'roi', 'brainnet' ] mode of calling res_defaults, either referring to initialization ('init'), loading ('load'), type of plot ('projection', 'simul', 'behav', 'conn', 'vbm', 'roi') or settings for toolbox ('brainnet') varargin [ name-value pairs ] additional parameters can be set via name-value pairs with dot notation supported (e.g., 'behav.weight.numtop', 20)","title":"Inputs"},{"location":"mfiles/res_defaults/#outputs","text":"res [ struct ] result structure that has been updated with defaults","title":"Outputs"},{"location":"mfiles/res_defaults/#examples","text":"% Example 1 res . dir . frwork = 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME' ; res . frwork . level = 1 ; res . env . fileend = '_1' ; res = res_defaults ( res , 'load' ); % Example 2 res = res_defaults ([], 'load' , 'dir.frwork' , ... 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME' ); % Example 3 res = res_defaults ([], 'load' , 'dir.frwork' , ... 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME' ); res = res_defaults ( res , 'behav' );","title":"Examples"},{"location":"mfiles/res_defaults/#fields-from-results","text":"You can find a description of the fields and subfields of res obtained during the experiment. For more transparency, all the root fields are inherited from cfg .","title":"Fields from results"},{"location":"mfiles/res_defaults/#dir","text":"Essential paths to your project, framework (subfields inherited from cfg , see here ) and the output folders of the experiment such as grid search, permutations and main results. The results folder by default contains a summary results table and three mat files, one for the res file, one for the hyperparameters used for the models on the training set (or combined training and validation set when using grid search), and one for the outputs of these models (including e.g. weights and holdout/test correlations). .project [ path ] full path to your project, such as 'PATH/TO/YOUR/PROJECT' .frwork [ path ] full path to your framework, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME' .grid [ path ] full path to your grid search results, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/grid/level ' .perm [ path ] full path to your permutation testing results, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/perm/level ' .res [ path ] full path to your main results, such as 'PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/grid/level '","title":"dir"},{"location":"mfiles/res_defaults/#frwork","text":"Results-orinted details of framework with different subfields as in cfg . .level [ int ] level of the multivariate associative effect in the iterative calculation process (often called 'mode' in the CCA literature) .split.all [ int or numeric array ] all splits at the current level .split.nall [ int ] number of all splits at the current level .split.sig [ int or numeric array ] all splits at the current level that passed significance .split.best best split based on the criterion defined by cfg.defl.crit (for details, see here )","title":"frwork"},{"location":"mfiles/res_defaults/#stat","text":"Results of statistical inference with some subfields inherited from cfg . For further details on the type of statistical inferences, see the description here . .nperm [ int ] number of permutations .split.pval [ int or numeric array ] uncorrected p-values of the statistical inference within splits (one p-value per split) of note, in case of 'omnibus' hypothesis the associative effect is significant if any of the p-values are smaller than 0.05 / number of splits .overall.pval [ int ] p-value of the statistical inference across splits of note, this is not provided for the statistical inference with 'omnibus' hypothesis as that requires only a Bonferroni correction but no p-values are calculated across splits","title":"stat"},{"location":"mfiles/res_defaults/#env","text":"Computation environment with the 'fileend' and 'save' subfields inherited from cfg . For further details see here . .fileend [ char --> '_1' ] suffix at the end of each file saved in the framework folder whilst running the experiment on a cluster for file storage efficiency and easier data transfer, we suggest to use 'cleanup_files.m' after an experiment is completed which deletes the unnecessary copies of the same file and replaces the 'fileend' to the default value of '_1' .save.compression [ boolean --> 0 ] defines if files are saved with or without compression of note, loading an uncompressed file can be faster for very large data files","title":"env"},{"location":"mfiles/res_defaults/#fields-and-options-for-plotting","text":"You can find a description of all possible settings of the fields and subfields of res for plotting below. First parameter always indicates the default option.","title":"Fields and options for plotting"},{"location":"mfiles/res_defaults/#gen","text":"General options for plotting. We recommend using 'interactive' file selection when new to plotting results to be able to understand what files are needed for the specific plots. Later, especially when wishing to automatize plots, it might be convenient to use 'none' file selection to avoid the interactive pop-up window and the files can be easily controlled/provided by setting the appropriate fields (e.g. 'file.label'). .selectfile [ 'none', 'interactive' ] file selection using a wrapper function over the select_file function of SPM .weight.flip [ boolean --> false ] choice of flipping the weights, i.e. changing their sign .weight.type [ 'weight', 'correlation' ] type/interpretation of weight, i.e. how much each feature/variable contributes to the associative effect 'weight' refers to the model weights read from model.mat , which is most often used in the PLS literature 'correlation' refers to the correlation between the input features/variables and the latent variables/projections (i.e., brain and behavioural scores), which is most often used in the CCA literature","title":"gen"},{"location":"mfiles/res_defaults/#data","text":"Subfields of 'fname' inherited from cfg . For further details see here . .X.fname, .Y.fname, .C.fname [ filepath --> 'X.mat', 'Y.mat', 'C.mat' ] file with full path to data X, Y, C","title":"data"},{"location":"mfiles/res_defaults/#param","text":"Options for plotting hyperparameter optimization results from grid search. For the plotting function, see plot_paropt . .xscale, .yscale [ 'lin', 'log' ] scale for x, y axis for RCCA, a logarithmic scale is recommended for SPLS, a linear scale is recommended","title":"param"},{"location":"mfiles/res_defaults/#proj","text":"Options for plotting projections of data (i.e. scores/latent variables). For the plotting function, see plot_proj . In this scatter plot each dot is a subject and as the axes are latent variables, it is also called as latent space plot. Most often, we plot brain score vs. behaviour score for a specific level (i.e., associative effect). However, it is possible to plot the latent space across multiple levels (and averaged over modalities in a given level after standardization). The latent space can be colour-coded by a continuous variable used as a colormap or a discrete variable with different colours for the different groups of data. In this case, a label file and a corresponding data file should be provided. .xlabel, .ylabel [ char --> 'Brain score', 'Behavioural score' ] label for x, y axis .xlim, y.lim [ numeric array --> NaN ] limit for x, y axis similar to MATLAB's built-in 'xlim', 'ylim' .file.label [ filepath --> 'LabelsY.xlsx' ] label file with full path for additional colormap/group information label file and data file (see below) should be corresponding to each other, i.e. row i in label file (without header) corresponds to column i in data file .file.data [ filepath --> 'Y.mat'; ] data file with full path for additional colormap/group information label file and data file (see above) should be corresponding to each other, i.e. row i in label file (without header) corresponds to column i in data file .flip [ boolean --> false ] choice of flipping the weights, i.e. changing their sign .multi_level [ boolean --> 0 ] choice for multi-level projection plots, i.e. averaging across modalities","title":"proj"},{"location":"mfiles/res_defaults/#behav","text":"Weight postprocessing options, label file and figure settings for plotting weights of labelled behavioural data as horizontal or vertical bar plots. For the general plotting function, see plot_weight . .weight.filtzero [ boolean --> 1 ] postprocess weights removing weights with zero values .weight.numtop [ int --> Inf ] postprocess weights by number of top weights 'Inf' refers to including all weights .weight.sorttype [ 'sign', ' ', 'abs' ] postprocess weights by sorting them in descending order 'sign' sorts both positive and negative weights in descending order (i.e., as if they were two independent lists) 'abs' sorts weights based on absolute value '' refers to no sorting .weight.cutlabels [ boolean --> 1 ] shorten labels that are too long .file.label [ filepath --> 'LabelsY.xlsx' ] label file with full path for Y data .label.maxchar [ int --> Inf ] maximum number of characters for label names in figure of note, use it when some of the labels are too long to display on the figure setting it to e.g. 50","title":"behav"},{"location":"mfiles/res_defaults/#conn","text":"Options for plotting connectivity data e.g. from resting-state fMRI. For the general plotting function, see plot_weight and for a template plotting connectivity data, see here . As brain data is in concatenated format (subject x feature matrix) and it might not include all pairwise connections, a mask file is necessary to be able to reshape the data to node x node format. A node can be an anatomical or functional Region of Interest (ROI) or an Independent Component Analyis (ICA) component. In the previous cases we recommend using BrainNet Viewer to plot weights on a glass brain as edges or summarized as nodes (for settings, see here ). In the latter case, we recommend using a schemaball plot (see plot_weight_brain_schemaball.m). It is also possible to summarize the weights as connections within/between modules and we use MATLAB's built-in imagesc function to plot the results. .file.mask [ filepath --> 'mask.mat' ] mask file with full path for connectivity data 'mask' variable is a matrix with booleans for the connections that are selected for brain data after concatenation (e.g. lower triangular part) .weight.filtzero [ boolean --> 1 ] postprocess weights removing weights with zero values .weight.numtop [ int --> Inf ] postprocess weights by number of top weights 'Inf' refers to including all weights .weight.sorttype [ 'sign', ' ', 'abs' ] postprocess weights by sorting them in descending order 'sign' sorts both positive and negative weights in descending order (i.e., as if they were two independent lists) 'abs' sorts weights based on absolute value '' refers to no sorting .weight.type [ 'auto', 'strength' ] postprocess weights by multiplying them by the sign of the population mean in the original data 'auto' does no postprocessing 'strength' does postprocessing .weight.sign [ 'all', 'positive', 'negative' ] postprocess weights by selecting a subset of them (e.g. with positive or negative sign) 'all' does no postprocessing .module.disp [ boolean --> 0 ] choice to display module weights in command line .module.type [ 'average', 'sum' ] calculate the average or sum of weights within/between modules .module.norm [ 'none', 'global', 'max' ] normalize module weights... 'none' does no normalization .file.label [ filepath --> 'LabelsX.xlsx' ] label file with full path for connectivity data","title":"conn"},{"location":"mfiles/res_defaults/#vbm","text":"Options for plotting Voxel Based Morphometry (VBM), i.e. voxel-wise data. For the general plotting function, see plot_weight and for a template plotting VBM data, see here . As brain data is in concatenated format (subject x feature matrix), a mask file is necessary to be able to write the weights in nii format. By default, the brain weights are assumed to be in MNI space, however, normalization is possible if given a template/source image and a rigid body transformation matrix to reorient the image to approximately match the MNI template space (otherwise, if the weight image is way out of MNI space, the normalization fails). We use BrainNet Viewer to plot the cortical weights on a glass brain (for settings, see here ). It is also possible to plot the brain weights separately for cortex and subcortex, in which case a nii atlas with a corresponding label file (in tabulated format with a column defining cortex/subcortex) is needed (see e.g. AAL ) to be able to obtain information about the cortical/subcortical position of each voxel. For plotting subcortical regions, Nilearn should be installed on the machine. .file.mask [ filepath --> 'mask.nii' ] mask file with full path for VBM data nii file includes an image with booleans for the voxels that are selected for brain data after concatenation .file.MNI [ filepath --> 'T1_1mm_brain.nii' ] template/source image with full path for normalization .transM [ numeric array --> eye(4) ] rigid body transformation matrix to reorient the weight image to MNI space before normalization occurs of note, for the current ABCD analyses it is [1 0 0 118.6; 0 1 0 -128.6; 0 0 1 -63.3; 0 0 0 1] .file.atlas.img [ filepath --> 'AAL2.nii' ] atlas nii file in MNI space for definition of cortex/subcortex of note, full path is not necessary if file is in path or 'aal' folder in the external folder .file.atlas.label [ filepath --> 'Labels_AAL2.xlsx' ] atlas label file for for definition of cortex/subcortex of note, full path is not necessary if file is in path or 'aal' folder in the external folder .subcortex [ boolean --> 0 ] choice to plot subcortical volume separately and remove it from cortical plot .file.nilearn.MNI [ filepath --> 'MNI152_T1_1mm_brain.nii' ] MNI template file with full path for background image in Nilearn of note, only relevant if 'res.vbm.subcortex' is 1","title":"vbm"},{"location":"mfiles/res_defaults/#roi","text":"Weight postprocessing options and label file for plotting weights of Region Of Interest (ROI) data. For the general plotting function, see plot_weight and for a template plotting ROI data, see here . The settings here refer to genuine ROI data entered into the experiment. Post-hoc ROI summarization will be added as a feature to VBM data later. .weight.filtzero [ boolean --> 1 ] postprocess weights removing weights with zero values .weight.numtop [ int --> Inf ] postprocess weights by number of top weights 'Inf' refers to including all weights .weight.sorttype [ 'sign', ' ', 'abs' ] postprocess weights by sorting them in descending order 'sign' sorts both positive and negative weights in descending order (i.e., as if they were two independent lists) 'abs' sorts weights based on absolute value '' refers to no sorting .file.label [ filepath --> 'LabelsX.xlsx' ] label file with full path for ROI data","title":"roi"},{"location":"mfiles/res_defaults/#simul","text":"Weight postprocessing options for plotting simulation results. For the general plotting function, see plot_weight and for an example plot, see here . .weight.filtzero [ boolean --> 0 ] postprocess weights removing weights with zero values .weight.numtop [ int --> Inf ] postprocess weights by number of top weights 'Inf' refers to including all weights .weight.sorttype [ ' ', 'sign', 'abs' ] postprocess weights by sorting them in descending order '' refers to no sorting 'sign' sorts both positive and negative weights in descending order (i.e., as if they were two independent lists) 'abs' sorts weights based on absolute value","title":"simul"},{"location":"mfiles/res_defaults/#brainnet","text":"Settings for BrainNet Viewer for automatic plotting of brain weights on a glass brain and saving it as bitmap image in file. Make sure that your data and the glass brain are in the same space, by default BrainNet Viewer uses MNI space. The options file includes configurations for BrainNet Viewer to use your preferred styling. In case such file does not exist BrainNet Viewer uses its default settings. The BrainNet Viewer GUI is not closed after the visualization is completed, so at first usage you can manually edit your preferred settings and save them as options file for future usage. .file.surf [ filepath --> 'BrainMesh_ICBM152.nv' ] brain mesh (glass brain) file with full path in external folder of note, full path is not necessary if file is in path or 'brainnet' folder in external folder .file.options [ filepath --> 'options.mat' ] options file with full path for BrainNet configuration See also: cfg_defaults Author: Agoston Mihalik Website: MLNL","title":"brainnet"},{"location":"mfiles/template_analysis_rcca/","text":"template_analysis1_rcca This is a template function. Please make a private copy of this file and change the necessary paths and specifications. Description This template uses RCCA with correlation as hyperparameter optimization criterion in a multiple holdout framework. Deflation is done using the best split (based on highest correlation) and CCA projection deflation (applied to all splits). Statistical inference is done at two stages. First, for each outer split permutation test is performed based on holdout correlations and using 1000 permutations. Second, for the inference across splits (i.e. if the associative effect is significant as a whole), 'omnibus' hypothesis is used, which tests if any outer split is significant after adjusting the threshold with Bonferroni correction (e.g. p=0.005 for 10 splits). To account for the dependence structure across subjects/examples, exchangeability blocks are used both for restricted partitioning and permutations (for details, see Winkler et al 2015 and the PALM toolbox ). Importantly, due to the efficient implementation of RCCA the code is able to run on local computers in a few hours especially on smaller datasets or on a computer with high specifications. Highlights CCA projection deflation using the best split and applied to all splits statistical inference for each individual split and across splits hyperparameter optimization criterion: test correlation dependency across examples: exchangeability blocks computation environment: local Usage Make a copy of this function and change your project folder. You can then run this template by calling the function without any input parameters from within the command window. template_analysis1_rcca() Configuration % Project folder cfg . dir . project = '/PATH/TO/YOUR/PROJECT/' ; % Machine settings cfg . machine . name = 'rcca' ; cfg . machine . metric = { 'correl' }; cfg . machine . param . crit = 'correl' ; % Environment settings cfg . env . comp = 'local' ; % Framework settings cfg . frwork . name = 'holdout' ; cfg . frwork . flag = '_corr' ; % Restricted CV and permutation settings cfg . data . block = 1 ; cfg . frwork . split . EBcol = 2 : 3 ; % Deflation settings cfg . defl . name = 'cca-projection' ; cfg . defl . crit = 'correl' ; cfg . defl . split = 'all' ; % Statistical inference cfg . stat . split . crit = 'correl' ; cfg . stat . overall . crit = 'none' ; % Number of permutation tests cfg . stat . nperm = 1000 ; Authors: Agoston Mihalik Website: MLNL","title":"Regularized CCA"},{"location":"mfiles/template_analysis_rcca/#description","text":"This template uses RCCA with correlation as hyperparameter optimization criterion in a multiple holdout framework. Deflation is done using the best split (based on highest correlation) and CCA projection deflation (applied to all splits). Statistical inference is done at two stages. First, for each outer split permutation test is performed based on holdout correlations and using 1000 permutations. Second, for the inference across splits (i.e. if the associative effect is significant as a whole), 'omnibus' hypothesis is used, which tests if any outer split is significant after adjusting the threshold with Bonferroni correction (e.g. p=0.005 for 10 splits). To account for the dependence structure across subjects/examples, exchangeability blocks are used both for restricted partitioning and permutations (for details, see Winkler et al 2015 and the PALM toolbox ). Importantly, due to the efficient implementation of RCCA the code is able to run on local computers in a few hours especially on smaller datasets or on a computer with high specifications.","title":"Description"},{"location":"mfiles/template_analysis_rcca/#highlights","text":"CCA projection deflation using the best split and applied to all splits statistical inference for each individual split and across splits hyperparameter optimization criterion: test correlation dependency across examples: exchangeability blocks computation environment: local","title":"Highlights"},{"location":"mfiles/template_analysis_rcca/#usage","text":"Make a copy of this function and change your project folder. You can then run this template by calling the function without any input parameters from within the command window. template_analysis1_rcca()","title":"Usage"},{"location":"mfiles/template_analysis_rcca/#configuration","text":"% Project folder cfg . dir . project = '/PATH/TO/YOUR/PROJECT/' ; % Machine settings cfg . machine . name = 'rcca' ; cfg . machine . metric = { 'correl' }; cfg . machine . param . crit = 'correl' ; % Environment settings cfg . env . comp = 'local' ; % Framework settings cfg . frwork . name = 'holdout' ; cfg . frwork . flag = '_corr' ; % Restricted CV and permutation settings cfg . data . block = 1 ; cfg . frwork . split . EBcol = 2 : 3 ; % Deflation settings cfg . defl . name = 'cca-projection' ; cfg . defl . crit = 'correl' ; cfg . defl . split = 'all' ; % Statistical inference cfg . stat . split . crit = 'correl' ; cfg . stat . overall . crit = 'none' ; % Number of permutation tests cfg . stat . nperm = 1000 ; Authors: Agoston Mihalik Website: MLNL","title":"Configuration"},{"location":"mfiles/template_analysis_rerun/","text":"template_analysis_rerun This is a template function. Please make a private copy of this file and change the necessary paths and specifications. Description To rerun a specific analysis, simply load the cfg.mat file that has been created by the toolbox. You can find the cfg.mat file inside '/PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/' Usage Make a copy of this function and change the path to the cfg file. You can then run this template by calling the function without any input parameters from within the command window. template_analysis_rerun() Authors: Agoston Mihalik Website: MLNL","title":"Rerun Analysis"},{"location":"mfiles/template_analysis_rerun/#description","text":"To rerun a specific analysis, simply load the cfg.mat file that has been created by the toolbox. You can find the cfg.mat file inside '/PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/'","title":"Description"},{"location":"mfiles/template_analysis_rerun/#usage","text":"Make a copy of this function and change the path to the cfg file. You can then run this template by calling the function without any input parameters from within the command window. template_analysis_rerun() Authors: Agoston Mihalik Website: MLNL","title":"Usage"},{"location":"mfiles/template_analysis_spls/","text":"template_spls_analysis1 This is a template function. Please make a private copy of this file and change the necessary paths and specifications. Description This template uses SPLS with correlation and weight similarity as hyperparameter optimization criteria in a multiple holdout framework. Deflation is done using the best split (based on lowest p-value and highest correlation if tie) and PLS-mode A deflation (applied to all splits). Statistical inference is done at two stages. First, for each outer split permutation test is performed based on holdout correlations and using 1000 permutations. Second, for the inference across splits (i.e. if the associative effect is significant as a whole), 'omnibus' hypothesis is used, which tests if any outer split is significant after adjusting the threshold with Bonferroni correction (e.g. p=0.005 for 10 splits). For a similar application, see Monteiro et al 2016 . Highlights PLS-mode A deflation using the best split and applied to all splits statistical inference for each individual split and across splits hyperparameter optimization criterion: test correlation and similarity of weights Usage Make a copy of this function and change your project folder. You can then run this template by calling the function without any input parameters from within the command window. template_analysis1_spls() Configuration % Project folder cfg . dir . project = '/PATH/TO/YOUR/PROJECT/' ; % Machine settings cfg . machine . metric = { 'correl' 'simwx' 'simwy' }; cfg . machine . param . crit = 'correl+simwxy' ; % Environment settings cfg . env . comp = 'cluster' ; % Framework settings cfg . frwork . name = 'holdout' ; cfg . frwork . flag = '_modeA_corr-simwxy' ; % Deflation settings cfg . defl . name = 'pls-modeA' ; cfg . defl . crit = 'pval+correl' ; cfg . defl . split = 'all' ; % Statistical inference cfg . stat . split . crit = 'correl' ; cfg . stat . overall . crit = 'none' ; % Number of permutation tests cfg . stat . nperm = 1000 ; Authors: Agoston Mihalik Website: MLNL","title":"Sparse PLS"},{"location":"mfiles/template_analysis_spls/#description","text":"This template uses SPLS with correlation and weight similarity as hyperparameter optimization criteria in a multiple holdout framework. Deflation is done using the best split (based on lowest p-value and highest correlation if tie) and PLS-mode A deflation (applied to all splits). Statistical inference is done at two stages. First, for each outer split permutation test is performed based on holdout correlations and using 1000 permutations. Second, for the inference across splits (i.e. if the associative effect is significant as a whole), 'omnibus' hypothesis is used, which tests if any outer split is significant after adjusting the threshold with Bonferroni correction (e.g. p=0.005 for 10 splits). For a similar application, see Monteiro et al 2016 .","title":"Description"},{"location":"mfiles/template_analysis_spls/#highlights","text":"PLS-mode A deflation using the best split and applied to all splits statistical inference for each individual split and across splits hyperparameter optimization criterion: test correlation and similarity of weights","title":"Highlights"},{"location":"mfiles/template_analysis_spls/#usage","text":"Make a copy of this function and change your project folder. You can then run this template by calling the function without any input parameters from within the command window. template_analysis1_spls()","title":"Usage"},{"location":"mfiles/template_analysis_spls/#configuration","text":"% Project folder cfg . dir . project = '/PATH/TO/YOUR/PROJECT/' ; % Machine settings cfg . machine . metric = { 'correl' 'simwx' 'simwy' }; cfg . machine . param . crit = 'correl+simwxy' ; % Environment settings cfg . env . comp = 'cluster' ; % Framework settings cfg . frwork . name = 'holdout' ; cfg . frwork . flag = '_modeA_corr-simwxy' ; % Deflation settings cfg . defl . name = 'pls-modeA' ; cfg . defl . crit = 'pval+correl' ; cfg . defl . split = 'all' ; % Statistical inference cfg . stat . split . crit = 'correl' ; cfg . stat . overall . crit = 'none' ; % Number of permutation tests cfg . stat . nperm = 1000 ; Authors: Agoston Mihalik Website: MLNL","title":"Configuration"},{"location":"mfiles/template_plot_conn/","text":"template_plot_conn This is a template function. Please make a private copy of this file and change the necessary paths and specifications. Description This is a template for plotting brain-behaviour analysis of resting-state fMRI and labelled behavioural data. For an application, see figures in Mihalik et al 2019 . Highlights plot hyperparameter surface plot projections plot behavioural weights plot brain connectivity weights Usage Make a copy of this function and change your project folder. You can then run this template by calling the function without any input parameters from within the command window. Run the function multiple times to plot figures for multiple levels. All plots are automatically saved to the results folder of your project ('/PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/res'), separated by different levels (i.e., associative effects). For detailed instructions of the specific plots, see plot_paropt , plot_proj and plot_weight under Basic plotting in the navigation bar. template_plot_conn() Authors: Agoston Mihalik Website: MLNL","title":"Connectivity"},{"location":"mfiles/template_plot_conn/#description","text":"This is a template for plotting brain-behaviour analysis of resting-state fMRI and labelled behavioural data. For an application, see figures in Mihalik et al 2019 .","title":"Description"},{"location":"mfiles/template_plot_conn/#highlights","text":"plot hyperparameter surface plot projections plot behavioural weights plot brain connectivity weights","title":"Highlights"},{"location":"mfiles/template_plot_conn/#usage","text":"Make a copy of this function and change your project folder. You can then run this template by calling the function without any input parameters from within the command window. Run the function multiple times to plot figures for multiple levels. All plots are automatically saved to the results folder of your project ('/PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/res'), separated by different levels (i.e., associative effects). For detailed instructions of the specific plots, see plot_paropt , plot_proj and plot_weight under Basic plotting in the navigation bar. template_plot_conn() Authors: Agoston Mihalik Website: MLNL","title":"Usage"},{"location":"mfiles/template_plot_projections/","text":"template_plot_projections This is a template function. Please make a private copy of this file and change the necessary paths and specifications. Description This is a template for plotting projections. This can be done in multiple ways, i.e. only for one level, for two levels, for different data modalities and so on. For more information, please see plot_proj Highlights plot projections Usage Make a copy of this function and change your framework folder. You can then run this template by calling the function without any input parameters from within the command window. Alternatively, you can simply run this template as a script file if you delete the function definition. Run script of function multiple times to plot figures for multiple levels. All plots are automatically saved to your project folder. You can find them inside your results folder, separated by different levels. template_plot_projections() Authors: Agoston Mihalik Website: MLNL","title":"Template plot projections"},{"location":"mfiles/template_plot_projections/#description","text":"This is a template for plotting projections. This can be done in multiple ways, i.e. only for one level, for two levels, for different data modalities and so on. For more information, please see plot_proj","title":"Description"},{"location":"mfiles/template_plot_projections/#highlights","text":"plot projections","title":"Highlights"},{"location":"mfiles/template_plot_projections/#usage","text":"Make a copy of this function and change your framework folder. You can then run this template by calling the function without any input parameters from within the command window. Alternatively, you can simply run this template as a script file if you delete the function definition. Run script of function multiple times to plot figures for multiple levels. All plots are automatically saved to your project folder. You can find them inside your results folder, separated by different levels. template_plot_projections() Authors: Agoston Mihalik Website: MLNL","title":"Usage"},{"location":"mfiles/template_plot_roi/","text":"template_plot_roi This is a template function. Please make a private copy of this file and change the necessary paths and specifications. Description This is a template for plotting brain-behaviour analysis of ROI-wise structural MRI and labelled behavioural data For an application, see figures in Mihalik et al 2019 . Highlights plot hyperparameter surface plot projections plot behavioural weights plot regional brain weights Usage Make a copy of this function and change your project folder. You can then run this template by calling the function without any input parameters from within the command window. Run the function multiple times to plot figures for multiple levels. All plots are automatically saved to the results folder of your project ('/PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/res'), separated by different levels (i.e., associative effects). For detailed instructions of the specific plots, see plot_paropt , plot_proj and plot_weight under Basic plotting in the navigation bar. template_plot_roi() Authors: Agoston Mihalik Website: MLNL","title":"ROI"},{"location":"mfiles/template_plot_roi/#description","text":"This is a template for plotting brain-behaviour analysis of ROI-wise structural MRI and labelled behavioural data For an application, see figures in Mihalik et al 2019 .","title":"Description"},{"location":"mfiles/template_plot_roi/#highlights","text":"plot hyperparameter surface plot projections plot behavioural weights plot regional brain weights","title":"Highlights"},{"location":"mfiles/template_plot_roi/#usage","text":"Make a copy of this function and change your project folder. You can then run this template by calling the function without any input parameters from within the command window. Run the function multiple times to plot figures for multiple levels. All plots are automatically saved to the results folder of your project ('/PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/res'), separated by different levels (i.e., associative effects). For detailed instructions of the specific plots, see plot_paropt , plot_proj and plot_weight under Basic plotting in the navigation bar. template_plot_roi() Authors: Agoston Mihalik Website: MLNL","title":"Usage"},{"location":"mfiles/template_plot_simulation/","text":"template_plot_simulation This is a template function. Please make a private copy of this file and change the necessary paths and specifications. Description This is a template for plotting simulation results using a single outer split Highlights plot projections plot weights Y as stem plot plot weights X as stem plot Usage Make a copy of this function and change your framework folder. You can then run this template by calling the function without any input parameters from within the command window. Alternatively, you can simply run this template as a script file if you delete the function definition. Run script of function multiple times to plot figures for multiple levels. All plots are automatically saved to your project folder. You can find them inside your results folder, separated by different levels. template_plot_simulation() Authors: Agoston Mihalik Website: MLNL","title":"Template plot simulation"},{"location":"mfiles/template_plot_simulation/#description","text":"This is a template for plotting simulation results using a single outer split","title":"Description"},{"location":"mfiles/template_plot_simulation/#highlights","text":"plot projections plot weights Y as stem plot plot weights X as stem plot","title":"Highlights"},{"location":"mfiles/template_plot_simulation/#usage","text":"Make a copy of this function and change your framework folder. You can then run this template by calling the function without any input parameters from within the command window. Alternatively, you can simply run this template as a script file if you delete the function definition. Run script of function multiple times to plot figures for multiple levels. All plots are automatically saved to your project folder. You can find them inside your results folder, separated by different levels. template_plot_simulation() Authors: Agoston Mihalik Website: MLNL","title":"Usage"},{"location":"mfiles/template_plot_vbm/","text":"template_plot_vbm This is a template function. Please make a private copy of this file and change the necessary paths and specifications. Description This is a template for plotting brain-behaviour analysis of voxel-wise (voxel-based morphometry, VBM) structural MRI and labelled behavioural data. For an application, see figures in Mihalik et al 2020 . Highlights plot hyperparameter surface plot projections plot behavioural weights plot cortical brain weights Usage Make a copy of this function and change your project folder. You can then run this template by calling the function without any input parameters from within the command window. Run the function multiple times to plot figures for multiple levels. All plots are automatically saved to the results folder of your project ('/PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/res'), separated by different levels (i.e., associative effects). For detailed instructions of the specific plots, see plot_paropt , plot_proj and plot_weight under Basic plotting in the navigation bar. template_plot_vbm() Authors: Agoston Mihalik Website: MLNL","title":"VBM"},{"location":"mfiles/template_plot_vbm/#description","text":"This is a template for plotting brain-behaviour analysis of voxel-wise (voxel-based morphometry, VBM) structural MRI and labelled behavioural data. For an application, see figures in Mihalik et al 2020 .","title":"Description"},{"location":"mfiles/template_plot_vbm/#highlights","text":"plot hyperparameter surface plot projections plot behavioural weights plot cortical brain weights","title":"Highlights"},{"location":"mfiles/template_plot_vbm/#usage","text":"Make a copy of this function and change your project folder. You can then run this template by calling the function without any input parameters from within the command window. Run the function multiple times to plot figures for multiple levels. All plots are automatically saved to the results folder of your project ('/PATH/TO/YOUR/PROJECT/framework/ANALYSIS_NAME/res'), separated by different levels (i.e., associative effects). For detailed instructions of the specific plots, see plot_paropt , plot_proj and plot_weight under Basic plotting in the navigation bar. template_plot_vbm() Authors: Agoston Mihalik Website: MLNL","title":"Usage"}]}